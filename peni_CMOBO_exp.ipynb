{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from botorch.utils import t_batch_mode_transform\n",
    "import torch\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models import  SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "from botorch.acquisition.monte_carlo import MCAcquisitionFunction\n",
    "from botorch.acquisition.monte_carlo import AcquisitionFunction\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "from botorch.optim.initializers import gen_batch_initial_conditions\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "\n",
    "#kernels\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel, MaternKernel\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem setting\n",
    "\n",
    "penicillin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.test_functions.multi_objective import Penicillin\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "test_f = Penicillin(negate = True)\n",
    "bounds = torch.tensor([[60,10,293,10,0.01,600,5],[120, 18, 303, 18, 0.1, 700, 6.5]], dtype= torch.float64)\n",
    "def generate_initial_data(n):\n",
    "    # generate training data\n",
    "    train_x = draw_sobol_samples(bounds=bounds, n=n, q=1).squeeze(1)\n",
    "    train_obj = test_f(train_x)\n",
    "    return train_x, train_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "import torch\n",
    "\n",
    "class HyperVolumeScalarizedUCB(AnalyticAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        beta: float,\n",
    "        theta: torch.Tensor,\n",
    "        ref: torch.Tensor,\n",
    "        maximize: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the HyperVolume Scalarized Upper Confidence Bound Acquisition Function.\n",
    "\n",
    "        Args:\n",
    "            model: A BoTorch model representing the posterior distribution of the objectives.\n",
    "            beta (Tensor of shape [1] or [o]): The exploration-exploitation trade-off parameter(s).\n",
    "            theta (Tensor of shape [o]): The weights used for scalarizing the upper bounds, where `o` is the number of objectives.\n",
    "            maximize (bool): Whether to maximize or minimize the scalarized objective. Defaults to True (maximize).\n",
    "        \"\"\"\n",
    "        super(AnalyticAcquisitionFunction, self).__init__(model)\n",
    "        self.maximize = maximize\n",
    "        self.register_buffer(\"beta\", torch.as_tensor(beta))\n",
    "        self.register_buffer(\"theta\", torch.as_tensor(theta))\n",
    "        self.register_buffer(\"ref\", torch.as_tensor(ref))\n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluate the scalarized Upper Confidence Bound on the candidate set X.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor of shape [b, d]): A tensor containing `(b)` batches of `d`-dimensional design points.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [b]: A tensor containing the scalarized Upper Confidence Bound values for each batch.\n",
    "        \"\"\"\n",
    "        self.beta = self.beta.to(X)\n",
    "        self.theta = self.theta.to(X)\n",
    "        self.ref = self.ref.to(X)\n",
    "        posterior = self.model.posterior(X)\n",
    "        means = posterior.mean.squeeze(dim=-2)  # b x o\n",
    "        std_devs = posterior.variance.squeeze(dim=-2).sqrt()  # b x o\n",
    "        m = means.shape[1]\n",
    "        # Calculate upper confidence bounds for each objective\n",
    "        u_t = means + (self.beta.expand_as(means) * std_devs) - self.ref # b x o\n",
    "\n",
    "        # Apply the scalarization function to the upper bounds\n",
    "        scalarized_ut = torch.min(((u_t / self.theta) ** m), dim=-1)[0]  # b\n",
    "\n",
    "        return scalarized_ut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary Acq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryAcq(MCAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        beta: float,\n",
    "        theta: torch.Tensor,\n",
    "        ref: torch.Tensor,\n",
    "        maximize: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        An auxiliary acquisition defined in Algo.2\n",
    "\n",
    "        Args:\n",
    "            model: A BoTorch model representing the posterior distribution of the objectives.\n",
    "            beta (Tensor of shape [1] or [o]): The exploration-exploitation trade-off parameter(s).\n",
    "            theta (Tensor of shape [o]): The weights used for scalarizing the upper bounds, where `o` is the number of objectives.\n",
    "            maximize (bool): Whether to maximize or minimize the scalarized objective. Defaults to True (maximize).\n",
    "        \"\"\"\n",
    "        super(MCAcquisitionFunction, self).__init__(model)\n",
    "        self.maximize = maximize\n",
    "        self.register_buffer(\"beta\", torch.as_tensor(beta))\n",
    "        self.register_buffer(\"theta\", torch.as_tensor(theta))\n",
    "        self.register_buffer(\"ref\", torch.as_tensor(ref))\n",
    "    @t_batch_mode_transform()\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluate the scalarized Upper Confidence Bound on the candidate set X.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor of shape [b, d]): A tensor containing `(b)` batches of `d`-dimensional design points.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [b]: A tensor containing the scalarized Upper Confidence Bound values for each batch.\n",
    "        \"\"\"\n",
    "        self.beta = self.beta.to(X)\n",
    "        self.theta = self.theta.to(X)\n",
    "        self.ref = self.ref.to(X)\n",
    "        posterior = self.model.posterior(X)\n",
    "        #print(posterior.mean.shape)\n",
    "        means = posterior.mean  # b x q x o\n",
    "        std_devs = posterior.variance.sqrt()  # b x q x o\n",
    "        # Calculate upper confidence bounds for each objective\n",
    "        u_t = means + (self.beta.expand_as(means) * std_devs) - self.ref # b x qx o\n",
    "        #print('233', u_t.shape)\n",
    "\n",
    "        # Apply the scalarization function to the upper bounds\n",
    "        scalarized_ut = torch.min(torch.min(u_t, dim=-1)[0], dim=-1)[0]  # b\n",
    "        return scalarized_ut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "def create_ucb_constraints(model, beta: float, thresholds: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Creates a list of non-linear inequality constraints for a multi-output GP model, ensuring that the upper confidence\n",
    "    bounds of the model's outputs are greater than or equal to the specified thresholds.\n",
    "\n",
    "    Args:\n",
    "        model (MultiTaskGP): A multi-output Gaussian Process model.\n",
    "        beta (float): The scalar coefficient for the variance component of the UCB.\n",
    "        thresholds (torch.Tensor): A tensor of thresholds for each output dimension.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Callable, bool]]: A list of tuples, each containing a callable constraint and a boolean indicating\n",
    "                                      whether the constraint is intra-point (True) or inter-point (False). Each callable\n",
    "                                      takes a tensor `X` of shape [q, d] (where `d` is the dimension of the input space\n",
    "                                      and `q` can be 1 or more representing different design points) and returns a scalar\n",
    "                                      that should be non-negative if the constraint is satisfied.\n",
    "    \"\"\"\n",
    "    \n",
    "    def constraint(X):\n",
    "        \"\"\"\n",
    "        Evaluates all constraints for a batch of design points.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): A tensor of shape [q, d] (where `d` is the dimension of the input space and `q` can be 1 or more \n",
    "                              representing different design points).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape [q, m] (where `m` is the number of output dimensions) containing the evaluated \n",
    "                          constraints.\n",
    "        \"\"\"\n",
    "        # Compute posterior at X\n",
    "        X = X.unsqueeze(0)\n",
    "        posterior = model.posterior(X)\n",
    "        mean = posterior.mean\n",
    "        variance = posterior.variance\n",
    "        ucb = mean + beta * variance.sqrt()  # Compute the UCB\n",
    "\n",
    "        # Evaluate all constraints and return the difference from thresholds\n",
    "        return ucb - thresholds\n",
    "\n",
    "    # Create a list of constraints for each output dimension, all set as intra-point since they evaluate individually\n",
    "    constraints = [(lambda X, i=i: constraint(X)[:, i], True) for i in range(thresholds.size(0))]\n",
    "\n",
    "    return constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample_on_n_sphere(N, R):\n",
    "    # Return a single sample of a vector of dimension N\n",
    "    # with a uniform distribution on the (N-1)-Sphere surface of radius R.\n",
    "    # RATIONALE: https://mathworld.wolfram.com/HyperspherePointPicking.html\n",
    "    \n",
    "    # Generate a normally distributed point\n",
    "    X = torch.randn(N)\n",
    "\n",
    "    # Normalize this point to the surface of the sphere, then scale by radius R\n",
    "    return R * X / torch.norm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BO loop\n",
    "\n",
    "take thresholds to be: 10, -60, -350\n",
    "\n",
    "approximated maximum HV(with 70 points): ~11000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel picking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trial-1: \n",
    "\n",
    "- mean: [-0.4168, 0.2521, 0.4669]\n",
    "\n",
    "- lengthscale of rbf: 0.2746 \n",
    "\n",
    "trial-2:\n",
    "- mean: [-0.0467, 0.1484, 0.1960]\n",
    "- lengthscale: 0.4914\n",
    "\n",
    "trial -3:\n",
    "- mean: [-0.3820, 0.2300, 0.4253]\n",
    "- lengthscale: 0.2439\n",
    "\n",
    "trial -4:\n",
    "Model 1 hyperparameters:\n",
    "- Lengthscale: 0.2611877935810223\n",
    "- Outputscale: 1.162574728363308\n",
    "- mean: [-0.5130541132169476, 0.3377964141882237, 0.5965205987363811]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from metrics import HV, violation\n",
    "from gpytorch.means.constant_mean import ConstantMean\n",
    "base = RBFKernel()\n",
    "base.lengthscale = 0.2611877935810223\n",
    "covar_module = ScaleKernel(\n",
    "base_kernel=base,\n",
    ")\n",
    "covar_module.outputscale = 1.162574728363308\n",
    "means = [-0.5130541132169476, 0.3377964141882237, 0.5965205987363811]\n",
    "#means = [  3.6342, -13.0406, -92.4280]\n",
    "mean_modules = []\n",
    "for i in range(3):\n",
    "    current_mean_module = ConstantMean()\n",
    "    current_mean_module.constant = means[i]\n",
    "    mean_modules.append(current_mean_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def voxel_grid_sampling_with_indices(points, voxel_size = 5.0):\n",
    "    # Calculate the minimum and maximum coordinates\n",
    "    min_coords = torch.min(points, dim=0).values\n",
    "    max_coords = torch.max(points, dim=0).values\n",
    "\n",
    "    # Shift points so that the minimum coordinates are at the origin\n",
    "    shifted_points = points - min_coords\n",
    "\n",
    "    # Quantize the points to voxel grid coordinates\n",
    "    voxel_indices = torch.floor(shifted_points / voxel_size).long()\n",
    "\n",
    "    # Use a dictionary to store unique voxel indices and the corresponding row index\n",
    "    voxel_dict = {}\n",
    "    for idx, voxel_idx in enumerate(voxel_indices):\n",
    "        voxel_idx_tuple = tuple(voxel_idx.tolist())\n",
    "        if voxel_idx_tuple not in voxel_dict:\n",
    "            voxel_dict[voxel_idx_tuple] = idx\n",
    "\n",
    "    # Extract the row indices of the sampled points\n",
    "    sampled_indices = torch.tensor(list(voxel_dict.values()))\n",
    "\n",
    "    return sampled_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000000000000000000000000000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round0: 3536.3583628954493\n",
      "round 1 3538.853867612411 random:  3536.3583628954493\n",
      "round 2 3567.0304539096614 random:  3934.1927719729338\n",
      "round 3 3567.0304539096614 random:  3934.1927719729338\n",
      "round 4 3580.666593736734 random:  3934.1927719729338\n",
      "round 5 3580.666593736734 random:  3934.1927719729338\n",
      "round 6 3580.666593736734 random:  3934.1927719729338\n",
      "round 7 3580.666593736734 random:  3934.1927719729338\n",
      "round 8 3580.666593736734 random:  3934.1927719729338\n",
      "round 9 3580.666593736734 random:  3934.1927719729338\n",
      "round 10 3580.7692373369237 random:  3934.1927719729338\n",
      "round 11 3580.7692373369237 random:  3934.1927719729338\n",
      "round 12 3580.7692373369237 random:  3934.1927719729338\n",
      "round 13 3763.1404888140632 random:  3971.9877233182574\n",
      "round 14 3812.2555557708574 random:  3971.9877233182574\n",
      "round 15 3824.0316371051667 random:  4004.727407066155\n",
      "round 16 3824.0316371051667 random:  4004.727407066155\n",
      "round 17 3824.0316371051667 random:  4004.727407066155\n",
      "round 18 3830.3734289436934 random:  4084.646846170062\n",
      "round 19 3830.3734289436934 random:  4084.646846170062\n",
      "round 20 3830.3734289436934 random:  4084.646846170062\n",
      "round 21 4127.365839423791 random:  4084.646846170062\n",
      "round 22 4130.310171609022 random:  4084.646846170062\n",
      "round 23 4130.750809684416 random:  4084.646846170062\n",
      "round 24 4286.033792250074 random:  4084.646846170062\n",
      "round 25 4305.468280837518 random:  4084.646846170062\n",
      "round 26 4305.468280837518 random:  4084.646846170062\n",
      "round 27 4305.468280837518 random:  4084.646846170062\n",
      "round 28 4355.410143194922 random:  4308.2423689393745\n",
      "round 29 4355.410143194922 random:  4308.2423689393745\n",
      "round 30 4554.607847957804 random:  4317.464169572669\n",
      "round 31 4554.607847957804 random:  4317.464169572669\n",
      "round 32 4571.356875386552 random:  4317.464169572669\n",
      "round 33 4572.962518669482 random:  4317.464169572669\n",
      "round 34 4640.699995394849 random:  4317.464169572669\n",
      "round 35 4640.699995394849 random:  4317.464169572669\n",
      "round 36 4640.699995394849 random:  4317.464169572669\n",
      "round 37 4640.699995394849 random:  4317.464169572669\n",
      "round 38 4640.699995394849 random:  4317.464169572669\n",
      "round 39 4640.699995394849 random:  4317.464169572669\n",
      "round 40 4650.358804105756 random:  4317.464169572669\n",
      "round 41 4650.358804105756 random:  4317.464169572669\n",
      "round 42 4650.358804105756 random:  4317.464169572669\n",
      "round 43 4730.782953396175 random:  4317.464169572669\n",
      "round 44 4730.782953396175 random:  4317.464169572669\n",
      "round 45 4730.782953396175 random:  4317.464169572669\n",
      "round 46 4743.989241802571 random:  4317.464169572669\n",
      "round 47 4743.989241802571 random:  4317.464169572669\n",
      "round 48 4743.989241802571 random:  4317.464169572669\n",
      "round 49 4743.989241802571 random:  4317.464169572669\n",
      "round 50 4790.774369518886 random:  4317.464169572669\n",
      "round 51 4790.774369518886 random:  4317.464169572669\n",
      "round 52 4790.774369518886 random:  4317.464169572669\n",
      "round 53 4790.774369518886 random:  4317.464169572669\n",
      "round 54 4790.774369518886 random:  4966.903019476124\n",
      "round 55 4790.774369518886 random:  4966.903019476124\n",
      "round 56 4790.774369518886 random:  4966.903019476124\n",
      "round 57 4790.774369518886 random:  4966.903019476124\n",
      "round 58 4790.774369518886 random:  4966.903019476124\n",
      "round 59 4806.36052211652 random:  4966.903019476124\n",
      "round 60 4806.36052211652 random:  4966.903019476124\n",
      "round 61 4866.1486830974045 random:  4966.903019476124\n",
      "round 62 4866.1486830974045 random:  4966.903019476124\n",
      "round 63 4866.1486830974045 random:  4966.903019476124\n",
      "round 64 4866.1486830974045 random:  4966.903019476124\n",
      "round 65 4866.1486830974045 random:  4966.903019476124\n",
      "round 66 4866.1486830974045 random:  4968.270023142285\n",
      "round 67 4866.1486830974045 random:  4968.270023142285\n",
      "round 68 5335.9260692208245 random:  4968.270023142285\n",
      "round 69 5335.9260692208245 random:  5496.078055773791\n",
      "round 70 5335.9260692208245 random:  5496.078055773791\n",
      "oround0: 5480.927172901254\n",
      "round 1 5480.927172901254 random:  5480.927172901254\n",
      "round 2 5480.927172901254 random:  5480.927172901254\n",
      "round 3 5480.927172901254 random:  5480.927172901254\n",
      "round 4 5502.883517320483 random:  5480.927172901254\n",
      "round 5 5502.883517320483 random:  5480.927172901254\n",
      "round 6 5502.883517320483 random:  5480.927172901254\n",
      "round 7 5502.883517320483 random:  5480.927172901254\n",
      "round 8 5502.883517320483 random:  5480.927172901254\n",
      "round 9 5503.02932810245 random:  5480.927172901254\n",
      "round 10 5503.02932810245 random:  5480.927172901254\n",
      "round 11 5515.100243367962 random:  5480.927172901254\n",
      "round 12 5515.100243367962 random:  5480.927172901254\n",
      "round 13 5603.558193780193 random:  5480.927172901254\n",
      "round 14 5760.933981747536 random:  5480.927172901254\n",
      "round 15 5760.933981747536 random:  5480.927172901254\n",
      "round 16 5772.373090188933 random:  5480.927172901254\n",
      "round 17 5772.373090188933 random:  5561.917575441739\n",
      "round 18 5772.373090188933 random:  5561.917575441739\n",
      "round 19 5772.373090188933 random:  5706.728595517625\n",
      "round 20 5772.373090188933 random:  5706.728595517625\n",
      "round 21 5772.373090188933 random:  5706.728595517625\n",
      "round 22 5772.373090188933 random:  5706.728595517625\n",
      "round 23 5772.373090188933 random:  5706.728595517625\n",
      "round 24 5772.373090188933 random:  5706.728595517625\n",
      "round 25 5772.373090188933 random:  5706.728595517625\n",
      "round 26 6881.816444065146 random:  5706.728595517625\n",
      "round 27 6881.816444065146 random:  5706.728595517625\n",
      "round 28 6881.816444065146 random:  5706.728595517625\n",
      "round 29 6881.816444065146 random:  5706.728595517625\n",
      "round 30 6892.521349710659 random:  5706.728595517625\n",
      "round 31 6912.133177760311 random:  5706.728595517625\n",
      "round 32 6912.133177760311 random:  5706.728595517625\n",
      "round 33 6912.133177760311 random:  5706.728595517625\n",
      "round 34 6912.133177760311 random:  5725.265021566596\n",
      "round 35 6912.133177760311 random:  5725.265021566596\n",
      "round 36 6912.133177760311 random:  5725.265021566596\n",
      "round 37 6951.745948517019 random:  5725.265021566596\n",
      "round 38 6951.745948517019 random:  5725.265021566596\n",
      "round 39 6951.745948517019 random:  5725.265021566596\n",
      "round 40 6951.745948517019 random:  5725.265021566596\n",
      "round 41 6951.745948517019 random:  5851.399698499167\n",
      "round 42 6951.745948517019 random:  5851.399698499167\n",
      "round 43 6964.936723563267 random:  5851.399698499167\n",
      "round 44 6964.936723563267 random:  5851.399698499167\n",
      "round 45 6964.936723563267 random:  5851.399698499167\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from platypus import NSGAII, Problem, Real, nondominated, experiment, calculate, display, ProcessPoolEvaluator\n",
    "c = 0\n",
    "thresholds = torch.tensor([10, -60, -350], dtype= torch.float64)\n",
    "print('0'*50)\n",
    "random_seeds = [83810, 14592, 3278, 97196, 36048, 32098, 29256, 18289, 96530, 13434, 88696, 97080, 71482, 11395, 77397, 55302, 4165, 3905, 12280, 28657, 30495, 66237, 78907, 3478, 73563,\n",
    "26062, 93850, 85181, 91924, 71426, 54987, 28893, 58878, 77236, 36463, 851, 99458, 20926, 91506, 55392, 44597, 36421, 20379, 28221, 44118, 13396, 12156, 49797, 12676, 47052]\n",
    "declared = False\n",
    "for seed in random_seeds[:10]:\n",
    "    torch.manual_seed(seed)\n",
    "    train_X, train_Y = generate_initial_data(64)\n",
    "    train_X = normalize(train_X, bounds)\n",
    "    #obtain ~20 evenly distributed objective points\n",
    "    resample_ind = voxel_grid_sampling_with_indices(train_Y)\n",
    "    train_X = train_X[resample_ind, :]\n",
    "    train_Y = train_Y[resample_ind, :]\n",
    "\n",
    "    train_Xr, train_Yr = train_X, train_Y\n",
    "    print(f'round{0}:', HV(Y = train_Y, ref = thresholds))\n",
    "    Hpv = []\n",
    "    Hpvr = []\n",
    "    Acq = []\n",
    "    NUM_ITER = 70\n",
    "    for batch in range(NUM_ITER):\n",
    "        t0 = time.monotonic()\n",
    "        model_list = []\n",
    "        m = 3\n",
    "        for i in range(m):\n",
    "            current_model = SingleTaskGP(train_X= train_X, train_Y= train_Y[:, i].unsqueeze(-1), outcome_transform= Standardize(m = 1), mean_module= mean_modules[i],covar_module=covar_module, train_Yvar= torch.zeros((train_X.shape[0],1)) + 0.01**2)\n",
    "            model_list.append(current_model)\n",
    "        model = ModelListGP(*model_list)\n",
    "        # mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "        # fit_gpytorch_mll(mll)\n",
    "        #t1 = time.monotonic()\n",
    "        #sample theta from distribution\n",
    "        theta = get_random_sample_on_n_sphere(m,1).abs()\n",
    "        beta = 0.1 * math.log((1+batch))\n",
    "        beta_const = beta\n",
    "        def acq_objective(X:list):\n",
    "            X= torch.tensor([X])\n",
    "            with torch.no_grad():\n",
    "                posterior = model.posterior(X)\n",
    "                mean = posterior.mean\n",
    "                std = posterior.variance.sqrt()\n",
    "                ucb_obj = mean + std * beta - thresholds\n",
    "                ucb_const = mean + std * beta_const - thresholds\n",
    "                acq = torch.min(torch.max(torch.zeros_like(ucb_obj), ucb_obj / theta) ** m, dim=-1)[0].cpu().tolist()\n",
    "                ucb_const = ucb_const.cpu().numpy().squeeze(0).tolist()\n",
    "                return acq, ucb_const\n",
    "        prob = Problem(7, 1, 3)\n",
    "        prob.types[:] = [Real(0,1)]*7\n",
    "        prob.constraints[:] = '>=0'\n",
    "        prob.function = acq_objective\n",
    "        prob.directions[:] = Problem.MAXIMIZE\n",
    "        algo = NSGAII(problem=prob, population_size= 1)\n",
    "        algo.run(1000)\n",
    "        candidate = torch.tensor([list(sol.variables) for sol in algo.result])\n",
    "        train_X = torch.cat([train_X, candidate],dim=0)\n",
    "        train_Y = torch.cat([train_Y, test_f(unnormalize(candidate, bounds= bounds))], dim = 0)\n",
    "        hv = HV(Y = train_Y, ref = thresholds)\n",
    "        Hpv.append(hv)\n",
    "\n",
    "        candidater, _ = generate_initial_data(1)\n",
    "        candidater = normalize(candidater, bounds)\n",
    "        train_Xr = torch.cat([train_Xr, candidater],dim=0)\n",
    "        train_Yr = torch.cat([train_Yr, test_f(unnormalize(candidater, bounds= bounds))], dim = 0)\n",
    "        hvr = HV(Y = train_Yr, ref = thresholds)\n",
    "        Hpvr.append(hvr)\n",
    "\n",
    "        #Acq.append(acq_scalar)\n",
    "        print(f'round {batch+1}',hv, 'random: ', hvr)\n",
    "    if not declared:\n",
    "        c+=1\n",
    "        torch.save(torch.tensor(Hpv), f'hv_peni_better_fit_2_{c}.pt')\n",
    "        #torch.save(torch.tensor(Acq), f'acq_peni_rbf{c}.pt')\n",
    "        torch.save(train_Y, f'obj_peni_better_fit_2_{c}.pt')\n",
    "        print('o', end='')\n",
    "    else:\n",
    "        print('*', end='')\n",
    "    declared = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1,len(Hpv)+1), Hpv)\n",
    "plt.plot(range(1,len(Hpvr)+1), Hpvr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3761, 7])\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# train_X, train_Y = generate_initial_data(10000)\n",
    "# train_X = normalize(train_X, bounds)\n",
    "# resample_ind = voxel_grid_sampling_with_indices(train_Y)\n",
    "# train_X = train_X[resample_ind, :]\n",
    "# train_Y = train_Y[resample_ind, :]\n",
    "# for i in range(19):\n",
    "#     ob_x, ob_y = generate_initial_data(10000)\n",
    "#     ob_x = normalize(ob_x, bounds)\n",
    "#     resample_ind = voxel_grid_sampling_with_indices(ob_y)\n",
    "#     ob_x = ob_x[resample_ind, :]\n",
    "#     ob_y = ob_y[resample_ind, :]\n",
    "#     train_X = torch.cat([train_X, ob_x], dim = 0)\n",
    "#     train_Y = torch.cat([train_Y, ob_y], dim = 0)\n",
    "# #obtain ~20 evenly distributed objective points\n",
    "# print(train_X.shape)\n",
    "# m = 3\n",
    "# model_list = []\n",
    "# for i in range(m):\n",
    "#     current_model = SingleTaskGP(train_X= train_X, train_Y= train_Y[:, i].unsqueeze(-1), outcome_transform= Standardize(m = 1), covar_module=covar_module, train_Yvar= torch.zeros((train_X.shape[0],1)) + 0.01**2, mean_module= mean_modules[i])\n",
    "#     model_list.append(current_model)\n",
    "# model = ModelListGP(*model_list)\n",
    "# mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "# fit_gpytorch_mll(mll)\n",
    "# print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1 hyperparameters:\n",
      "  Lengthscale: 0.2611877935810223\n",
      "  Outputscale: 1.162574728363308\n",
      "  mean: -0.5130541132169476\n",
      "\n",
      "Model 2 hyperparameters:\n",
      "  Lengthscale: 0.2611877935810223\n",
      "  Outputscale: 1.162574728363308\n",
      "  mean: 0.3377964141882237\n",
      "\n",
      "Model 3 hyperparameters:\n",
      "  Lengthscale: 0.2611877935810223\n",
      "  Outputscale: 1.162574728363308\n",
      "  mean: 0.5965205987363811\n"
     ]
    }
   ],
   "source": [
    "# for i, model in enumerate(model.models):\n",
    "#     print(f\"\\nModel {i+1} hyperparameters:\")\n",
    "#     lengthscale = model.covar_module.base_kernel.lengthscale\n",
    "#     outputscale = model.covar_module.outputscale\n",
    "#     mean = model.mean_module.constant\n",
    "#     print(f\"  Lengthscale: {lengthscale.item()}\")\n",
    "#     print(f\"  Outputscale: {outputscale.item()}\")\n",
    "#     print(f\"  mean: {mean.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom , obj= generate_initial_data(2000)\n",
    "dom = normalize(dom, bounds)\n",
    "with torch.no_grad():\n",
    "    posterior = model.posterior(dom)\n",
    "    mean = posterior.mean\n",
    "    srd = posterior.variance.sqrt()\n",
    "fig, ax = plt.subplots(1,3, figsize = (15, 6))\n",
    "pair = [[0,1], [1,2], [0,2]]\n",
    "for i in range(3):\n",
    "    ls = pair[i]\n",
    "    a = ls[0]\n",
    "    b = ls[1]\n",
    "    th1 = [5, -20, -100][a]\n",
    "    th2 = [5, -20, -100][b]\n",
    "    ax[i].scatter(mean[:,a].cpu().numpy(),mean[:,b].cpu().numpy(),s = 1,alpha = 1)\n",
    "    ax[i].scatter(obj[:,a].cpu().numpy(),obj[:,b].cpu().numpy(),s = 0.1, alpha = 1)\n",
    "    ax[i].scatter(train_Y[:,a], train_Y[:,b],s = 0.1, c ='red')\n",
    "    ax[i].set_xlabel(f'y{a+1}')\n",
    "    ax[i].set_ylabel(f'y{b+1}')\n",
    "plt.legend(['mean', 'objective', 'CMOBO_pareto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trial-1: \n",
    "\n",
    "- mean: [-0.4168, 0.2521, 0.4669]\n",
    "\n",
    "- lengthscale of rbf: 0.2746 \n",
    "\n",
    "trial-2:\n",
    "- mean: [-0.0467, 0.1484, 0.1960]\n",
    "- lengthscale: 0.4914\n",
    "\n",
    "trial -3:\n",
    "- mean: [-0.3820, 0.2300, 0.4253]\n",
    "- lengthscale: 0.2439\n",
    "\n",
    "trial -4:\n",
    "Model 1 hyperparameters:\n",
    "- Lengthscale: 0.2611877935810223\n",
    "- Outputscale: 1.162574728363308\n",
    "- mean: [-0.5130541132169476, 0.3377964141882237, 0.5965205987363811]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.models[0].covar_module.base_kernel.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mean[:,0].cpu().numpy(),mean[:,1].cpu().numpy(),s = 0.1)\n",
    "plt.scatter(obj[:,0].cpu().numpy(),obj[:,1].cpu().numpy(),s = 0.1)\n",
    "plt.xlabel('y1')\n",
    "plt.ylabel('y2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mean[:,1].cpu().numpy(),mean[:,2].cpu().numpy(),s = 0.1)\n",
    "plt.scatter(obj[:,1].cpu().numpy(),obj[:,2].cpu().numpy(),s = 0.1)\n",
    "plt.xlabel('y2')\n",
    "plt.ylabel('y3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.max(dim = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
