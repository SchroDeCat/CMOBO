{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "customOutput": null,
        "executionStartTime": 1668651350300,
        "executionStopTime": 1668651350308,
        "originalKey": "f27224aa-b567-4a6d-b6b3-74f2ecbfe319",
        "requestMsgId": "df1b7814-2d71-4421-b832-e10d0c1e7743"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), '..', 'toolkits'))\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "tkwargs = {\n",
        "    \"dtype\": torch.double,\n",
        "    \"device\": torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\"),\n",
        "}\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "89f8b99f-5cb2-45c9-9df6-7e1d18d4f8c6",
        "showInput": false
      },
      "source": [
        "### Problem setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "customOutput": null,
        "executionStartTime": 1668651350608,
        "executionStopTime": 1668651354486,
        "originalKey": "4227f250-60b5-4c97-b04c-3cfe7a1c410a",
        "requestMsgId": "83e67907-72c3-4bb8-8468-7eb99e616730"
      },
      "outputs": [],
      "source": [
        "from design import Design\n",
        "test_f = Design()\n",
        "def problem(X):\n",
        "    return test_f.evaluate(X.to(torch.float64))\n",
        "bounds = test_f.bounds\n",
        "d = 4\n",
        "M = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651354720,
        "executionStopTime": 1668651354729,
        "hidden_ranges": [],
        "originalKey": "192b8d87-b2e3-4223-b193-6399b8643391",
        "requestMsgId": "55d97599-5be9-4a7a-857c-18a9b56bf07d"
      },
      "outputs": [],
      "source": [
        "from botorch.models.gp_regression import SingleTaskGP\n",
        "from botorch.models.model_list_gp_regression import ModelListGP\n",
        "from botorch.models.transforms.outcome import Standardize\n",
        "from botorch.utils.sampling import draw_sobol_samples\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
        "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
        "def evaluate_slack(X, ref = torch.tensor([-1.9,-2.25]).to(**tkwargs)):\n",
        "    Y = problem(X)\n",
        "    vio_raw = Y -ref\n",
        "    return (vio_raw).sum(dim = -1, keepdim = True)\n",
        "\n",
        "def generate_initial_data(n):\n",
        "    # generate training data\n",
        "    train_x = draw_sobol_samples(bounds=bounds, n=n, q=1).squeeze(1)\n",
        "    train_obj, train_con = problem(train_x)\n",
        "    # negative values imply feasibility in botorch\n",
        "    # train_con = -evaluate_slack(train_x)\n",
        "    return train_x, train_obj, -train_con\n",
        "\n",
        "# base = RBFKernel()\n",
        "# covar_module = ScaleKernel(\n",
        "# base_kernel=base,\n",
        "# )\n",
        "def initialize_model(train_x, train_obj, train_con):\n",
        "    # define models for objective and constraint\n",
        "    train_x = normalize(train_x, bounds)\n",
        "    train_y = torch.cat([train_obj, train_con], dim=-1)\n",
        "    models = []\n",
        "    for i in range(train_y.shape[-1]):\n",
        "        models.append(\n",
        "            SingleTaskGP(\n",
        "                train_x, train_y[..., i : i + 1], outcome_transform=Standardize(m=1),train_Yvar= torch.zeros((train_x.shape[0],1)) + 0.05**2)\n",
        "            )\n",
        "        \n",
        "    model = ModelListGP(*models)\n",
        "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
        "    return mll, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651354970,
        "executionStopTime": 1668651355060,
        "hidden_ranges": [],
        "originalKey": "65dcfbb2-f1e9-40a1-9807-8cdc1cc3fdc8",
        "requestMsgId": "68a072df-7e90-4c7f-9915-520ca48c5e0a"
      },
      "outputs": [],
      "source": [
        "from botorch.acquisition.multi_objective.monte_carlo import (\n",
        "    qNoisyExpectedHypervolumeImprovement,\n",
        ")\n",
        "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n",
        "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
        "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
        "from botorch.utils.sampling import sample_simplex\n",
        "\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_RESTARTS = 4\n",
        "RAW_SAMPLES =4\n",
        "\n",
        "standard_bounds = torch.zeros(2, 4, **tkwargs)\n",
        "standard_bounds[1] = 1\n",
        "\n",
        "\n",
        "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, train_con, sampler):\n",
        "    \"\"\"Optimizes the qNEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
        "    train_x = normalize(train_x, bounds)\n",
        "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
        "        model=model,\n",
        "        ref_point= torch.tensor([-8,-8]),  # use known reference point\n",
        "        X_baseline=train_x,\n",
        "        sampler=sampler,\n",
        "        prune_baseline=False,\n",
        "        # define an objective that specifies which outcomes are the objectives\n",
        "        objective=IdentityMCMultiOutputObjective(outcomes=[0, 1]),\n",
        "        # specify that the constraint is on the last outcome\n",
        "        constraints=[\n",
        "                lambda Z: Z[..., -1],  # First constraint\n",
        "                lambda Z: Z[..., -2],  # Second constraint\n",
        "                lambda Z: Z[..., -3],  # Third constraint\n",
        "                lambda Z: Z[..., -4],  # Fourth constraint\n",
        "            ],\n",
        "    )\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=standard_bounds,\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
        "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
        "        sequential=True,\n",
        "    )\n",
        "    # observe new values\n",
        "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
        "    new_obj, new_con = problem(new_x)  + torch.randn_like(problem(new_x)) * 0.05\n",
        "    # negative values imply feasibility in botorch\n",
        "    return new_x, new_obj, -new_con"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651356028,
        "executionStopTime": 1668651959470,
        "hidden_ranges": [],
        "originalKey": "4c225d99-6425-4201-ac4a-a042a351c1d3",
        "requestMsgId": "be831f3c-ff7c-4c00-a215-fb021f0c5770"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import warnings\n",
        "\n",
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.exceptions import BadInitialCandidatesWarning\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
        "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "random_seeds = [83810, 14592, 3278, 97196, 36048, 32098, 29256, 18289, 96530, 13434, 88696, 97080, 71482, 11395, 77397, 55302, 4165, 3905, 12280, 28657, 30495, 66237, 78907, 3478, 73563,\n",
        "26062, 93850, 85181, 91924, 71426, 54987, 28893, 58878, 77236, 36463, 851, 99458, 20926, 91506, 55392, 44597, 36421, 20379, 28221, 44118, 13396, 12156, 49797, 12676, 47052]\n",
        "declared = False\n",
        "c = 0\n",
        "N_BATCH = 100\n",
        "MC_SAMPLES = 128 if not SMOKE_TEST else 16\n",
        "verbose = True\n",
        "for seed in random_seeds[:10]:\n",
        "    torch.manual_seed(seed)\n",
        "    train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi = generate_initial_data(10)\n",
        "    train_x_qnehvi = train_x_qnehvi.to(torch.float64)\n",
        "    hv = Hypervolume(ref_point=torch.tensor([-8,-8]).to(**tkwargs))\n",
        "    hvs_qnehvi, hvs_random = [], []\n",
        "    # call helper functions to generate initial training data and initialize model\n",
        "\n",
        "\n",
        "\n",
        "    train_x_random, train_obj_random, train_con_random = (\n",
        "        train_x_qnehvi,\n",
        "        train_obj_qnehvi,\n",
        "        train_con_qnehvi,\n",
        "    )\n",
        "\n",
        "    mll_qnehvi, model_qnehvi = initialize_model(\n",
        "        train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi\n",
        "    )\n",
        "\n",
        "    # compute pareto front\n",
        "    is_feas = (train_con_qnehvi <= 0).all(dim=-1)\n",
        "    feas_train_obj = train_obj_qnehvi[is_feas]\n",
        "    if feas_train_obj.shape[0] > 0:\n",
        "        pareto_mask = is_non_dominated(feas_train_obj)\n",
        "        pareto_y = feas_train_obj[pareto_mask]\n",
        "        # compute hypervolume\n",
        "        volume = hv.compute(pareto_y)\n",
        "    else:\n",
        "        volume = 0.0\n",
        "\n",
        "    # hvs_qnehvi.append(volume)\n",
        "    # hvs_random.append(volume)\n",
        "    # run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "    for iteration in range(1, N_BATCH + 1):\n",
        "        t0 = time.monotonic()\n",
        "\n",
        "        # fit the models\n",
        "        fit_gpytorch_mll(mll_qnehvi)\n",
        "\n",
        "        # define the qParEGO and qNEHVI acquisition modules using a QMC sampler\n",
        "        qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
        "\n",
        "        # optimize acquisition functions and get new observations\n",
        "        new_x_qnehvi, new_obj_qnehvi, new_con_qnehvi = optimize_qnehvi_and_get_observation(\n",
        "            model_qnehvi, train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi, qnehvi_sampler\n",
        "        )\n",
        "        new_x_random, new_obj_random, new_con_random = generate_initial_data(n=BATCH_SIZE)\n",
        "\n",
        "        # update training points\n",
        "        train_x_qnehvi = torch.cat([train_x_qnehvi, new_x_qnehvi])\n",
        "        train_obj_qnehvi = torch.cat([train_obj_qnehvi, new_obj_qnehvi])\n",
        "        train_con_qnehvi = torch.cat([train_con_qnehvi, new_con_qnehvi])\n",
        "\n",
        "        train_x_random = torch.cat([train_x_random, new_x_random])\n",
        "        train_obj_random = torch.cat([train_obj_random, new_obj_random])\n",
        "        train_con_random = torch.cat([train_con_random, new_con_random])\n",
        "\n",
        "        # update progress\n",
        "        for hvs_list, train_obj, train_con in zip(\n",
        "            (hvs_random, hvs_qnehvi),\n",
        "            (train_obj_random, train_obj_qnehvi),\n",
        "            (train_con_random,  train_con_qnehvi),\n",
        "        ):\n",
        "            # compute pareto front\n",
        "            is_feas = (train_con <= 0).all(dim=-1)\n",
        "            feas_train_obj = train_obj[is_feas]\n",
        "            if feas_train_obj.shape[0] > 0:\n",
        "                pareto_mask = is_non_dominated(feas_train_obj)\n",
        "                pareto_y = feas_train_obj[pareto_mask]\n",
        "                # compute feasible hypervolume\n",
        "                volume = hv.compute(pareto_y)\n",
        "            else:\n",
        "                volume = 0.0\n",
        "            hvs_list.append(volume)\n",
        "\n",
        "        # reinitialize the models so they are ready for fitting on next iteration\n",
        "        # Note: we find improved performance from not warm starting the model hyperparameters\n",
        "        # using the hyperparameters from the previous iteration\n",
        "        mll_qnehvi, model_qnehvi = initialize_model(\n",
        "            train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi\n",
        "        )\n",
        "\n",
        "        t1 = time.monotonic()\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"\\nBatch {iteration:>2}: Hypervolume (random, qNEHVI) = \"\n",
        "                f\"({hvs_random[-1]:>8f}, {hvs_qnehvi[-1]:>8f}), \"\n",
        "                f\"time = {t1-t0:>4.2f}.\",\n",
        "                end=\"\",\n",
        "            )\n",
        "        else:\n",
        "            print(\".\", end=\"\")\n",
        "    c+=1\n",
        "    vio = torch.where(train_con_qnehvi > 0, train_con_qnehvi, torch.zeros_like(train_con_qnehvi)).sum(dim = -1)\n",
        "    torch.save(torch.tensor(hvs_qnehvi), f'hv_design_ehvi_{c}.pt')\n",
        "    torch.save(vio, f'vio_design_ehvi_{c}.pt')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
