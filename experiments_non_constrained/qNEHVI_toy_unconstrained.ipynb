{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false,
        "customOutput": null,
        "executionStartTime": 1668649461840,
        "executionStopTime": 1668649461848,
        "originalKey": "41c30177-379b-4e63-9996-41bc17d70769",
        "requestMsgId": "7e4820a5-df3b-45a6-9826-42541ee0f4f4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "tkwargs = {\n",
        "    \"dtype\": torch.double,\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "}\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "8d895a93-397c-4d2c-b6f5-96f589312538",
        "showInput": false
      },
      "source": [
        "### Problem setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false,
        "customOutput": null,
        "executionStartTime": 1668649462081,
        "executionStopTime": 1668649462087,
        "originalKey": "a8741d41-72b7-42e8-bd5d-3972be9995f7",
        "requestMsgId": "bceaa0af-ffe5-415a-a5b1-93fc9cf7f71a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def f_1(x):\n",
        "    r = 1/x[:,0]+ x[:,1]\n",
        "    return(-r.unsqueeze(1))\n",
        "\n",
        "def f_2(x):\n",
        "    r = x[:,0]+ x[:,1]**2\n",
        "    return(-r.unsqueeze(1))\n",
        "\n",
        "def problem(X):\n",
        "    return torch.cat([f_1(X), f_2(X)], dim = -1)\n",
        "bounds = torch.tensor([[1]*2, [1.5]*2]).to( **tkwargs)\n",
        "\n",
        "\n",
        "d = 2\n",
        "M = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false,
        "customOutput": null,
        "executionStartTime": 1668649462312,
        "executionStopTime": 1668649462318,
        "originalKey": "47170c31-65e4-4f2d-949c-d91544e065fc",
        "requestMsgId": "8a44b7cb-cc5f-419e-849f-2b4d1dc3abe1"
      },
      "outputs": [],
      "source": [
        "from botorch.models.gp_regression import SingleTaskGP\n",
        "from botorch.models.model_list_gp_regression import ModelListGP\n",
        "from botorch.models.transforms.outcome import Standardize\n",
        "from botorch.utils.sampling import draw_sobol_samples\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
        "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
        "NOISE_SE = torch.tensor([0.1]*M).to(**tkwargs) \n",
        "def generate_initial_data(n):\n",
        "    # generate training data\n",
        "    train_x = draw_sobol_samples(bounds=bounds, n=n, q=1).squeeze(1)\n",
        "    train_obj_true = problem(train_x)\n",
        "    train_obj = train_obj_true + torch.randn_like(train_obj_true) * NOISE_SE\n",
        "    # negative values imply feasibility in botorch\n",
        "    # train_con = -evaluate_slack(train_x)\n",
        "    return train_x, train_obj, train_obj_true\n",
        "\n",
        "base = RBFKernel()\n",
        "covar_module = ScaleKernel(\n",
        "base_kernel=base,\n",
        ")\n",
        "# def initialize_model(train_x, train_obj, train_con):\n",
        "#     # define models for objective and constraint\n",
        "#     train_x = normalize(train_x, bounds)\n",
        "#     train_y = torch.cat([train_obj, train_con], dim=-1)\n",
        "#     models = []\n",
        "#     for i in range(train_y.shape[-1]):\n",
        "#         models.append(\n",
        "#             SingleTaskGP(\n",
        "#                 train_x, train_y[..., i : i + 1], outcome_transform=Standardize(m=1),train_Yvar= torch.zeros((train_x.shape[0],1)) + 0.01**2,covar_module = covar_module)\n",
        "#             )\n",
        "        \n",
        "#     model = ModelListGP(*models)\n",
        "#     mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
        "#     return mll, model\n",
        "\n",
        "def initialize_model(train_x, train_obj):\n",
        "    # define models for objective and constraint\n",
        "    train_x = normalize(train_x, bounds)\n",
        "    models = []\n",
        "    for i in range(train_obj.shape[-1]):\n",
        "        train_y = train_obj[..., i : i + 1]\n",
        "        train_yvar = torch.full_like(train_y, NOISE_SE[i] ** 2)\n",
        "        models.append(\n",
        "            SingleTaskGP(\n",
        "                train_x, train_y, outcome_transform=Standardize(m=1),train_Yvar= torch.zeros((train_x.shape[0],1)) + 0.01,covar_module = covar_module\n",
        "            )\n",
        "        )\n",
        "    model = ModelListGP(*models)\n",
        "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
        "    return mll, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false,
        "customOutput": null,
        "executionStartTime": 1668649462539,
        "executionStopTime": 1668649462641,
        "originalKey": "b7effe94-1327-405d-9148-c8e93470b846",
        "requestMsgId": "40a5edda-3bee-43c9-b84b-c669c288eb80"
      },
      "outputs": [],
      "source": [
        "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
        "from botorch.acquisition.objective import GenericMCObjective\n",
        "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
        "from botorch.utils.multi_objective.box_decompositions.non_dominated import (\n",
        "    FastNondominatedPartitioning,\n",
        ")\n",
        "from botorch.acquisition.multi_objective.monte_carlo import (\n",
        "    qExpectedHypervolumeImprovement,\n",
        "    qNoisyExpectedHypervolumeImprovement,\n",
        ")\n",
        "from botorch.utils.sampling import sample_simplex\n",
        "\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
        "RAW_SAMPLES = 512 if not SMOKE_TEST else 4\n",
        "\n",
        "standard_bounds = torch.zeros(2, d, **tkwargs)\n",
        "standard_bounds[1] = 1\n",
        "\n",
        "\n",
        "def optimize_qehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
        "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
        "    # partition non-dominated space into disjoint rectangles\n",
        "    with torch.no_grad():\n",
        "        pred = model.posterior(normalize(train_x, bounds)).mean\n",
        "    partitioning = FastNondominatedPartitioning(\n",
        "        ref_point=torch.tensor([-3.0,-4.0]),\n",
        "        Y=pred,\n",
        "    )\n",
        "    acq_func = qExpectedHypervolumeImprovement(\n",
        "        model=model,\n",
        "        ref_point=torch.tensor([-3.0,-4.0]),\n",
        "        partitioning=partitioning,\n",
        "        sampler=sampler,\n",
        "    )\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=standard_bounds,\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
        "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
        "        sequential=True,\n",
        "    )\n",
        "    # observe new values\n",
        "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
        "    new_obj_true = problem(new_x)\n",
        "    new_obj = new_obj_true + torch.randn_like(new_obj_true) *NOISE_SE\n",
        "    return new_x, new_obj, new_obj_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false,
        "customOutput": null,
        "executionStartTime": 1668649462860,
        "executionStopTime": 1668649462867,
        "originalKey": "f2749e5f-eb94-4150-8f52-057f90e08b39",
        "requestMsgId": "f9e71b3c-2eb6-4b79-92c5-72d16d368233"
      },
      "outputs": [],
      "source": [
        "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
        "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
        "    # partition non-dominated space into disjoint rectangles\n",
        "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
        "        model=model,\n",
        "        ref_point=torch.tensor([-3.0,-4.0]).tolist(),  # use known reference point\n",
        "        X_baseline=normalize(train_x, bounds),\n",
        "        prune_baseline=True,  # prune baseline points that have estimated zero probability of being Pareto optimal\n",
        "        sampler=sampler,\n",
        "    )\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=standard_bounds,\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
        "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
        "        sequential=True,\n",
        "    )\n",
        "    # observe new values\n",
        "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
        "    new_obj_true = problem(new_x)\n",
        "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
        "    return new_x, new_obj, new_obj_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false,
        "customOutput": null,
        "executionStartTime": 1668649463087,
        "executionStopTime": 1668649463185,
        "originalKey": "806b115f-a15f-44df-b7f9-d2f098969e02",
        "requestMsgId": "514d162f-78e0-447a-a483-c923cba18b80"
      },
      "outputs": [],
      "source": [
        "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
        "\n",
        "\n",
        "def optimize_qnparego_and_get_observation(model, train_x, train_obj, sampler):\n",
        "    \"\"\"Samples a set of random weights for each candidate in the batch, performs sequential greedy optimization\n",
        "    of the qNParEGO acquisition function, and returns a new candidate and observation.\"\"\"\n",
        "    train_x = normalize(train_x, bounds)\n",
        "    with torch.no_grad():\n",
        "        pred = model.posterior(train_x).mean\n",
        "    acq_func_list = []\n",
        "    for _ in range(BATCH_SIZE):\n",
        "        weights = sample_simplex(problem.num_objectives, **tkwargs).squeeze()\n",
        "        objective = GenericMCObjective(\n",
        "            get_chebyshev_scalarization(weights=weights, Y=pred)\n",
        "        )\n",
        "        acq_func = qNoisyExpectedImprovement(  # pyre-ignore: [28]\n",
        "            model=model,\n",
        "            objective=objective,\n",
        "            X_baseline=train_x,\n",
        "            sampler=sampler,\n",
        "            prune_baseline=True,\n",
        "        )\n",
        "        acq_func_list.append(acq_func)\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf_list(\n",
        "        acq_function_list=acq_func_list,\n",
        "        bounds=standard_bounds,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
        "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
        "    )\n",
        "    # observe new values\n",
        "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
        "    new_obj_true = problem(new_x)\n",
        "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
        "    return new_x, new_obj, new_obj_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def voxel_grid_sampling_with_indices(points, voxel_size = 5.0):\n",
        "    # Calculate the minimum and maximum coordinates\n",
        "    min_coords = torch.min(points, dim=0).values\n",
        "    max_coords = torch.max(points, dim=0).values\n",
        "\n",
        "    # Shift points so that the minimum coordinates are at the origin\n",
        "    shifted_points = points - min_coords\n",
        "\n",
        "    # Quantize the points to voxel grid coordinates\n",
        "    voxel_indices = torch.floor(shifted_points / voxel_size).long()\n",
        "\n",
        "    # Use a dictionary to store unique voxel indices and the corresponding row index\n",
        "    voxel_dict = {}\n",
        "    for idx, voxel_idx in enumerate(voxel_indices):\n",
        "        voxel_idx_tuple = tuple(voxel_idx.tolist())\n",
        "        if voxel_idx_tuple not in voxel_dict:\n",
        "            voxel_dict[voxel_idx_tuple] = idx\n",
        "\n",
        "    # Extract the row indices of the sampled points\n",
        "    sampled_indices = torch.tensor(list(voxel_dict.values()))\n",
        "\n",
        "    return sampled_indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "customOutput": null,
        "executionStartTime": 1668649463513,
        "executionStopTime": 1668649856754,
        "originalKey": "c29b731a-64e7-401d-a5b2-3879d8d39327",
        "requestMsgId": "43e1021f-9ecd-4d2c-a7ba-21e26051ce66"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import warnings\n",
        "\n",
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.exceptions import BadInitialCandidatesWarning\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "from botorch.utils.multi_objective.box_decompositions.dominated import (\n",
        "    DominatedPartitioning,\n",
        ")\n",
        "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "random_seeds = [83810, 14592, 3278, 97196, 36048, 32098, 29256, 18289, 96530, 13434, 88696, 97080, 71482, 11395, 77397, 55302, 4165, 3905, 12280, 28657, 30495, 66237, 78907, 3478, 73563,\n",
        "26062, 93850, 85181, 91924, 71426, 54987, 28893, 58878, 77236, 36463, 851, 99458, 20926, 91506, 55392, 44597, 36421, 20379, 28221, 44118, 13396, 12156, 49797, 12676, 47052]\n",
        "declared = False\n",
        "\n",
        "N_BATCH = 100\n",
        "MC_SAMPLES = 128 if not SMOKE_TEST else 16\n",
        "verbose = True\n",
        "c = 0\n",
        "\n",
        "verbose = True\n",
        "for seed in random_seeds[:10]:\n",
        "    torch.manual_seed(seed)\n",
        "    train_x_qnehvi, train_obj_qnehvi, train_obj_true_qnehvi= generate_initial_data(10)\n",
        "    # resample_ind = voxel_grid_sampling_with_indices(train_obj_qnehvi)\n",
        "    # train_x_qnehvi = train_x_qnehvi[resample_ind, :]\n",
        "    # train_obj_qnehvi = train_obj_qnehvi[resample_ind, :]\n",
        "    # train_con_qnehvi = train_con_qnehvi[resample_ind, :]\n",
        "    hvs_qnehvi = []\n",
        "\n",
        "    # # call helper functions to generate initial training data and initialize model\n",
        "    # train_x_qparego, train_obj_qparego, train_obj_true_qparego = generate_initial_data(\n",
        "    #     n=64\n",
        "    # )\n",
        "    # mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
        "\n",
        "    # train_x_qehvi, train_obj_qehvi, train_obj_true_qehvi = (\n",
        "    #     train_x_qparego,\n",
        "    #     train_obj_qparego,\n",
        "    #     train_obj_true_qparego,\n",
        "    # )\n",
        "    # train_x_qnehvi, train_obj_qnehvi, train_obj_true_qnehvi = (\n",
        "    #     train_x_qparego,\n",
        "    #     train_obj_qparego,\n",
        "    #     train_obj_true_qparego,\n",
        "    # )\n",
        "    # train_x_random, train_obj_random, train_obj_true_random = (\n",
        "    #     train_x_qparego,\n",
        "    #     train_obj_qparego,\n",
        "    #     train_obj_true_qparego,\n",
        "    # )\n",
        "    # mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
        "    mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi)\n",
        "\n",
        "    # compute hypervolume\n",
        "    bd = DominatedPartitioning(ref_point=torch.tensor([-1.9,-2.25]), Y=train_obj_true_qnehvi)\n",
        "    volume = bd.compute_hypervolume().item()\n",
        "\n",
        "    # hvs_qparego.append(volume)\n",
        "    # hvs_qehvi.append(volume)\n",
        "    hvs_qnehvi.append(volume)\n",
        "\n",
        "    # run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "    for iteration in range(1, N_BATCH + 1):\n",
        "\n",
        "        t0 = time.monotonic()\n",
        "\n",
        "        # fit the models\n",
        "        # fit_gpytorch_mll(mll_qparego)\n",
        "        # fit_gpytorch_mll(mll_qehvi)\n",
        "        fit_gpytorch_mll(mll_qnehvi)\n",
        "\n",
        "        # define the qEI and qNEI acquisition modules using a QMC sampler\n",
        "        # qparego_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
        "        # qehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
        "        qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
        "\n",
        "        # # optimize acquisition functions and get new observations\n",
        "        # (\n",
        "        #     new_x_qparego,\n",
        "        #     new_obj_qparego,\n",
        "        #     new_obj_true_qparego,\n",
        "        # ) = optimize_qnparego_and_get_observation(\n",
        "        #     model_qparego, train_x_qparego, train_obj_qparego, qparego_sampler\n",
        "        # )\n",
        "        # new_x_qehvi, new_obj_qehvi, new_obj_true_qehvi = optimize_qehvi_and_get_observation(\n",
        "        #     model_qehvi, train_x_qehvi, train_obj_qehvi, qehvi_sampler\n",
        "        # )\n",
        "        (\n",
        "            new_x_qnehvi,\n",
        "            new_obj_qnehvi,\n",
        "            new_obj_true_qnehvi,\n",
        "        ) = optimize_qnehvi_and_get_observation(\n",
        "            model_qnehvi, train_x_qnehvi, train_obj_qnehvi, qnehvi_sampler\n",
        "        )\n",
        "        # new_x_random, new_obj_random, new_obj_true_random = generate_initial_data(\n",
        "        #     n=BATCH_SIZE\n",
        "        # )\n",
        "\n",
        "        # update training points\n",
        "        # train_x_qparego = torch.cat([train_x_qparego, new_x_qparego])\n",
        "        # train_obj_qparego = torch.cat([train_obj_qparego, new_obj_qparego])\n",
        "        # train_obj_true_qparego = torch.cat([train_obj_true_qparego, new_obj_true_qparego])\n",
        "\n",
        "        # train_x_qehvi = torch.cat([train_x_qehvi, new_x_qehvi])\n",
        "        # train_obj_qehvi = torch.cat([train_obj_qehvi, new_obj_qehvi])\n",
        "        # train_obj_true_qehvi = torch.cat([train_obj_true_qehvi, new_obj_true_qehvi])\n",
        "\n",
        "        train_x_qnehvi = torch.cat([train_x_qnehvi, new_x_qnehvi])\n",
        "        train_obj_qnehvi = torch.cat([train_obj_qnehvi, new_obj_qnehvi])\n",
        "        train_obj_true_qnehvi = torch.cat([train_obj_true_qnehvi, new_obj_true_qnehvi])\n",
        "\n",
        "        # train_x_random = torch.cat([train_x_random, new_x_random])\n",
        "        # train_obj_random = torch.cat([train_obj_random, new_obj_random])\n",
        "        # train_obj_true_random = torch.cat([train_obj_true_random, new_obj_true_random])\n",
        "\n",
        "        # update progress\n",
        "        bd = DominatedPartitioning(ref_point=torch.tensor([-1.9,-2.25]), Y=train_obj_true_qnehvi)\n",
        "        volume = bd.compute_hypervolume().item()\n",
        "        hvs_qnehvi.append(volume)\n",
        "\n",
        "        # reinitialize the models so they are ready for fitting on next iteration\n",
        "        # Note: we find improved performance from not warm starting the model hyperparameters\n",
        "        # using the hyperparameters from the previous iteration\n",
        "        # mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
        "        # mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
        "        mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi)\n",
        "\n",
        "        t1 = time.monotonic()\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"({hvs_qnehvi[-1]:>4.2f}), \"\n",
        "                f\"time = {t1-t0:>4.2f}.\",\n",
        "                end=\"\",\n",
        "            )\n",
        "        else:\n",
        "            print(\".\", end=\"\")\n",
        "    c+=1\n",
        "    torch.save( hvs_qnehvi, f'toy_unconstrained_hv_{c}.pt')\n",
        "    torch.save(train_obj_true_qnehvi, f'toy_unconstrained_obj_true_{c}.pt')\n",
        "    torch.save(train_obj_qnehvi, f'toy_unconstrained_obj_{c}.pt')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
