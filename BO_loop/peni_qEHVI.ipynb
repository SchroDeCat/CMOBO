{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "20bce0d3-6ea0-4db3-bc3b-2569027a0e3b",
        "showInput": false
      },
      "source": [
        "## Constrained, Parallel, Multi-Objective BO in BoTorch with qNEHVI, and qParEGO\n",
        "\n",
        "In this tutorial, we illustrate how to implement a constrained multi-objective (MO) Bayesian Optimization (BO) closed loop in BoTorch.\n",
        "\n",
        "In general, we recommend using [Ax](https://ax.dev) for a simple BO setup like this one, since this will simplify your setup (including the amount of code you need to write) considerably. See [here](https://ax.dev/tutorials/multiobjective_optimization.html) for an Ax tutorial on MOBO. If desired, you can use a custom BoTorch model in Ax, following the [Using BoTorch with Ax](./custom_botorch_model_in_ax) tutorial. Given a `MultiObjective`, Ax will default to the $q$NEHVI acquisiton function. If desired, this can also be customized by adding `\"botorch_acqf_class\": <desired_botorch_acquisition_function_class>,` to the `model_kwargs`.\n",
        "\n",
        "We use the parallel ParEGO ($q$ParEGO) [1] and parallel Noisy Expected Hypervolume Improvement ($q$NEHVI) [2]  acquisition functions to optimize a synthetic C2-DTLZ2 test function with $M=2$ objectives, $V=1$ constraint, and $d=4$ parameters. The two objectives are\n",
        "$$f_1(\\mathbf x) = (1+ g(\\mathbf x_M))\\cos\\big(\\frac{\\pi}{2}x_1\\big)$$\n",
        "$$f_2(\\mathbf x) = (1+ g(\\mathbf x_M))\\sin\\big(\\frac{\\pi}{2}x_1\\big)$$\n",
        "where $g(\\mathbf x) = \\sum_{x_i \\in \\mathbf x_M} (x_i - 0.5)^2, \\mathbf x \\in [0,1]^d,$ and $\\mathbf x_M$ represents the last $d - M +1$ elements of $\\mathbf x$. Additionally, the C2-DTLZ2 problem uses the following constraint:\n",
        "\n",
        "$$c(\\mathbf x) = - \\min \\bigg[\\min_{i=1}^M\\bigg((f_i(\\mathbf x) -1 )^2 + \\sum_{j=1, j=i}^M (f_j^2 - r^2) \\bigg), \\bigg(\\sum_{i=1}^M \\big((f_i(\\mathbf x) - \\frac{1}{\\sqrt{M}})^2 - r^2\\big)\\bigg)\\bigg]\\geq 0$$\n",
        "\n",
        "where $\\mathbf x \\in [0,1]^d$ and $r=0.2$. \n",
        "\n",
        "The goal here is to *minimize* both objectives. Since BoTorch assumes maximization, we maximize the negative of each objective. Since there typically is no single best solution in multi-objective optimization problems, we seek to find the pareto frontier, the set of optimal trade-offs where improving one metric means deteriorating another.\n",
        "\n",
        "[1] [S. Daulton, M. Balandat, and E. Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. Advances in Neural Information Processing Systems 33, 2020.](https://arxiv.org/abs/2006.05078)\n",
        "\n",
        "[2] [S. Daulton, M. Balandat, and E. Bakshy. Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement. Advances in Neural Information Processing Systems 34, 2021.](https://arxiv.org/abs/2105.08195)\n",
        "\n",
        "**For batch optimization (or in noisy settings), we strongly recommend using $q$NEHVI rather than $q$EHVI [1] because it is far more efficient than $q$EHVI and mathematically equivalent in the noiseless setting.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "00d9f0ef-fab6-463e-a54d-a548581bc7f4",
        "showInput": false
      },
      "source": [
        "### Set dtype and device\n",
        "Note: $q$EHVI aggressively exploits parallel hardware and is much faster when run on a GPU. See [1] for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "customOutput": null,
        "executionStartTime": 1668651350300,
        "executionStopTime": 1668651350308,
        "originalKey": "f27224aa-b567-4a6d-b6b3-74f2ecbfe319",
        "requestMsgId": "df1b7814-2d71-4421-b832-e10d0c1e7743"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "tkwargs = {\n",
        "    \"dtype\": torch.double,\n",
        "    \"device\": torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\"),\n",
        "}\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "89f8b99f-5cb2-45c9-9df6-7e1d18d4f8c6",
        "showInput": false
      },
      "source": [
        "### Problem setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "customOutput": null,
        "executionStartTime": 1668651350608,
        "executionStopTime": 1668651354486,
        "originalKey": "4227f250-60b5-4c97-b04c-3cfe7a1c410a",
        "requestMsgId": "83e67907-72c3-4bb8-8468-7eb99e616730"
      },
      "outputs": [],
      "source": [
        "from botorch.test_functions.multi_objective import Penicillin\n",
        "\n",
        "problem = Penicillin(negate= True).to(**tkwargs)\n",
        "problem.bounds = torch.tensor([[60,10,293,10,0.01,600,5],[120, 18, 303, 18, 0.1, 700, 6.5]], dtype= torch.float64)\n",
        "problem.ref_point =torch.tensor([10, -60, -350]).to(**tkwargs)\n",
        "d = 7\n",
        "M = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "hidden_ranges": [],
        "originalKey": "2de9fbab-9d15-4410-8371-d3b1f730e3d7",
        "showInput": false
      },
      "source": [
        "#### Model initialization\n",
        "\n",
        "We use a multi-output `SingleTaskGP` to model the two objectives with a homoskedastic Gaussian likelihood with an inferred noise level.\n",
        "\n",
        "The models are initialized with $2(d+1)=10$ points drawn randomly from $[0,1]^{4}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651354720,
        "executionStopTime": 1668651354729,
        "hidden_ranges": [],
        "originalKey": "192b8d87-b2e3-4223-b193-6399b8643391",
        "requestMsgId": "55d97599-5be9-4a7a-857c-18a9b56bf07d"
      },
      "outputs": [],
      "source": [
        "from botorch.models.gp_regression import SingleTaskGP\n",
        "from botorch.models.model_list_gp_regression import ModelListGP\n",
        "from botorch.models.transforms.outcome import Standardize\n",
        "from botorch.utils.sampling import draw_sobol_samples\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
        "def evaluate_slack(X, ref = torch.tensor([10, -60, -350]).to(**tkwargs)):\n",
        "    Y = problem(X)\n",
        "    vio_raw = Y -ref\n",
        "    return (vio_raw).sum(dim = -1, keepdim = True)\n",
        "\n",
        "def generate_initial_data(n):\n",
        "    # generate training data\n",
        "    train_x = draw_sobol_samples(bounds=problem.bounds, n=n, q=1).squeeze(1)\n",
        "    train_obj = problem(train_x)\n",
        "    # negative values imply feasibility in botorch\n",
        "    train_con = -evaluate_slack(train_x)\n",
        "    return train_x, train_obj, train_con\n",
        "\n",
        "\n",
        "def initialize_model(train_x, train_obj, train_con):\n",
        "    # define models for objective and constraint\n",
        "    train_x = normalize(train_x, problem.bounds)\n",
        "    train_y = torch.cat([train_obj, train_con], dim=-1)\n",
        "    models = []\n",
        "    for i in range(train_y.shape[-1]):\n",
        "        models.append(\n",
        "            SingleTaskGP(\n",
        "                train_x, train_y[..., i : i + 1], outcome_transform=Standardize(m=1)\n",
        "            )\n",
        "        )\n",
        "    model = ModelListGP(*models)\n",
        "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
        "    return mll, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "hidden_ranges": [],
        "originalKey": "a0f6fa02-0843-4c45-8a19-8654226c1edc",
        "showInput": false
      },
      "source": [
        "#### Define a helper function that performs the essential BO step for $q$NEHVI\n",
        "The helper function below initializes the $q$NEHVI acquisition function, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. \n",
        "\n",
        "For this example, we'll use a small batch of $q=2$. Passing the keyword argument `sequential=True` to the function `optimize_acqf`specifies that candidates should be optimized in a sequential greedy fashion (see [1] for details why this is important). A simple initialization heuristic is used to select the 10 restart initial locations from a set of 512 random points. Multi-start optimization of the acquisition function is performed using LBFGS-B with exact gradients computed via auto-differentiation.\n",
        "\n",
        "**Reference Point**\n",
        "\n",
        "$q$NEHVI requires specifying a reference point, which is the lower bound on the objectives used for computing hypervolume. In this tutorial, we assume the reference point is known. In practice the reference point can be set 1) using domain knowledge to be slightly worse than the lower bound of objective values, where the lower bound is the minimum acceptable value of interest for each objective, or 2) using a dynamic reference point selection strategy.\n",
        "\n",
        "**Integrating over function values at in-sample designs**\n",
        "\n",
        "$q$NEHVI integrates over the unknown function values at the previously evaluated designs (see [2] for details). Therefore, we need to provide the previously evaluated designs (`train_x`, *normalized* to be within $[0,1]^d$) to the acquisition function.\n",
        "\n",
        "**Pruning baseline designs**\n",
        "To speed up integration over the function values at the previously evaluated designs, we prune the set of previously evaluated designs (by setting `prune_baseline=True`) to only include those which have positive probability of being on the current in-sample Pareto frontier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651354970,
        "executionStopTime": 1668651355060,
        "hidden_ranges": [],
        "originalKey": "65dcfbb2-f1e9-40a1-9807-8cdc1cc3fdc8",
        "requestMsgId": "68a072df-7e90-4c7f-9915-520ca48c5e0a"
      },
      "outputs": [],
      "source": [
        "from botorch.acquisition.multi_objective.monte_carlo import (\n",
        "    qNoisyExpectedHypervolumeImprovement,\n",
        ")\n",
        "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n",
        "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
        "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
        "from botorch.utils.sampling import sample_simplex\n",
        "\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
        "RAW_SAMPLES = 512 if not SMOKE_TEST else 4\n",
        "\n",
        "standard_bounds = torch.zeros(2, problem.dim, **tkwargs)\n",
        "standard_bounds[1] = 1\n",
        "\n",
        "\n",
        "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, train_con, sampler):\n",
        "    \"\"\"Optimizes the qNEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
        "    train_x = normalize(train_x, problem.bounds)\n",
        "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
        "        model=model,\n",
        "        ref_point= problem.ref_point.tolist(),  # use known reference point\n",
        "        X_baseline=train_x,\n",
        "        sampler=sampler,\n",
        "        prune_baseline=False,\n",
        "        # define an objective that specifies which outcomes are the objectives\n",
        "        objective=IdentityMCMultiOutputObjective(outcomes=[0, 1,2]),\n",
        "        # specify that the constraint is on the last outcome\n",
        "        constraints=[lambda Z: Z[..., -1]],\n",
        "    )\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=standard_bounds,\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
        "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
        "        sequential=True,\n",
        "    )\n",
        "    # observe new values\n",
        "    new_x = unnormalize(candidates.detach(), bounds=problem.bounds)\n",
        "    new_obj = problem(new_x)\n",
        "    # negative values imply feasibility in botorch\n",
        "    new_con = -evaluate_slack(new_x)\n",
        "    return new_x, new_obj, new_con"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "d4487ba0-4fad-41dd-a2ae-e0b95094dba1",
        "showInput": false
      },
      "source": [
        "### Perform Bayesian Optimization loop with $q$EHVI and $q$ParEGO\n",
        "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps:\n",
        "1. given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$\n",
        "2. observe $f(x)$ for each $x$ in the batch \n",
        "3. update the surrogate model. \n",
        "\n",
        "\n",
        "Just for illustration purposes, we run one trial with `N_BATCH=20` rounds of optimization. The acquisition function is approximated using `MC_SAMPLES=128` samples.\n",
        "\n",
        "*Note*: Running this may take a little while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651356028,
        "executionStopTime": 1668651959470,
        "hidden_ranges": [],
        "originalKey": "4c225d99-6425-4201-ac4a-a042a351c1d3",
        "requestMsgId": "be831f3c-ff7c-4c00-a215-fb021f0c5770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch  1: Hypervolume (random, qNEHVI) = (4572.24, 4572.24), time = 6.40.\n",
            "Batch  2: Hypervolume (random, qNEHVI) = (4572.24, 4580.86), time = 21.55.\n",
            "Batch  3: Hypervolume (random, qNEHVI) = (4859.56, 4640.25), time = 6.68.\n",
            "Batch  4: Hypervolume (random, qNEHVI) = (4859.56, 4640.25), time = 8.64.\n",
            "Batch  5: Hypervolume (random, qNEHVI) = (4859.56, 4640.25), time = 42.37.\n",
            "Batch  6: Hypervolume (random, qNEHVI) = (4859.56, 4640.25), time = 11.70.\n",
            "Batch  7: Hypervolume (random, qNEHVI) = (4859.56, 4640.25), time = 9.22.\n",
            "Batch  8: Hypervolume (random, qNEHVI) = (4859.56, 4640.25), time = 19.18.\n",
            "Batch  9: Hypervolume (random, qNEHVI) = (4859.56, 4640.25), time = 9.75.\n",
            "Batch 10: Hypervolume (random, qNEHVI) = (4945.28, 4640.25), time = 7.48.\n",
            "Batch 11: Hypervolume (random, qNEHVI) = (4945.28, 4640.25), time = 4.73.\n",
            "Batch 12: Hypervolume (random, qNEHVI) = (4945.28, 4640.25), time = 6.95.\n",
            "Batch 13: Hypervolume (random, qNEHVI) = (4945.28, 4640.25), time = 8.26.\n",
            "Batch 14: Hypervolume (random, qNEHVI) = (4945.28, 4640.25), time = 15.65.\n",
            "Batch 15: Hypervolume (random, qNEHVI) = (4945.28, 4640.25), time = 10.54.\n",
            "Batch 16: Hypervolume (random, qNEHVI) = (4945.28, 4640.25), time = 7.42.\n",
            "Batch 17: Hypervolume (random, qNEHVI) = (4945.28, 4956.83), time = 8.74.\n",
            "Batch 18: Hypervolume (random, qNEHVI) = (4945.28, 4956.83), time = 13.60.\n",
            "Batch 19: Hypervolume (random, qNEHVI) = (4945.28, 5132.66), time = 12.72.\n",
            "Batch 20: Hypervolume (random, qNEHVI) = (5087.74, 5132.66), time = 10.46.\n",
            "Batch 21: Hypervolume (random, qNEHVI) = (5087.74, 5784.53), time = 7.16.\n",
            "Batch 22: Hypervolume (random, qNEHVI) = (5087.74, 5784.53), time = 7.56.\n",
            "Batch 23: Hypervolume (random, qNEHVI) = (5087.74, 5784.53), time = 4.83.\n",
            "Batch 24: Hypervolume (random, qNEHVI) = (5087.74, 5784.53), time = 9.22.\n",
            "Batch 25: Hypervolume (random, qNEHVI) = (5087.74, 5796.52), time = 9.31.\n",
            "Batch 26: Hypervolume (random, qNEHVI) = (5087.74, 5796.52), time = 9.59.\n",
            "Batch 27: Hypervolume (random, qNEHVI) = (5087.74, 5796.52), time = 16.14.\n",
            "Batch 28: Hypervolume (random, qNEHVI) = (5087.74, 5796.52), time = 4.57.\n",
            "Batch 29: Hypervolume (random, qNEHVI) = (5087.74, 5796.52), time = 6.65.\n",
            "Batch 30: Hypervolume (random, qNEHVI) = (5087.74, 5796.52), time = 8.41.\n",
            "Batch 31: Hypervolume (random, qNEHVI) = (5087.74, 5796.52), time = 20.25.\n",
            "Batch 32: Hypervolume (random, qNEHVI) = (5087.74, 5796.52), time = 19.95.\n",
            "Batch 33: Hypervolume (random, qNEHVI) = (5087.74, 5901.14), time = 6.56.\n",
            "Batch 34: Hypervolume (random, qNEHVI) = (5094.54, 5901.14), time = 9.92.\n",
            "Batch 35: Hypervolume (random, qNEHVI) = (5094.54, 5901.14), time = 6.27.\n",
            "Batch 36: Hypervolume (random, qNEHVI) = (5094.54, 5901.14), time = 5.77.\n",
            "Batch 37: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 9.42.\n",
            "Batch 38: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 5.15.\n",
            "Batch 39: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 6.30.\n",
            "Batch 40: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 4.66.\n",
            "Batch 41: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 8.24.\n",
            "Batch 42: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 8.15.\n",
            "Batch 43: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 8.55.\n",
            "Batch 44: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 8.53.\n",
            "Batch 45: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 5.54.\n",
            "Batch 46: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 12.86.\n",
            "Batch 47: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 6.89.\n",
            "Batch 48: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 4.16.\n",
            "Batch 49: Hypervolume (random, qNEHVI) = (5133.95, 6369.50), time = 14.48.\n",
            "Batch 50: Hypervolume (random, qNEHVI) = (5133.95, 6382.89), time = 4.13.\n",
            "Batch 51: Hypervolume (random, qNEHVI) = (5133.95, 6382.89), time = 5.87.\n",
            "Batch 52: Hypervolume (random, qNEHVI) = (5133.95, 6382.89), time = 8.61.\n",
            "Batch 53: Hypervolume (random, qNEHVI) = (5133.95, 6382.89), time = 3.98.\n",
            "Batch 54: Hypervolume (random, qNEHVI) = (5133.95, 6382.89), time = 5.03.\n",
            "Batch 55: Hypervolume (random, qNEHVI) = (5133.95, 6382.89), time = 3.63.\n",
            "Batch 56: Hypervolume (random, qNEHVI) = (5133.95, 6730.84), time = 4.11.\n",
            "Batch 57: Hypervolume (random, qNEHVI) = (5133.95, 6932.61), time = 27.95.\n",
            "Batch 58: Hypervolume (random, qNEHVI) = (5133.95, 6932.61), time = 6.76.\n",
            "Batch 59: Hypervolume (random, qNEHVI) = (5133.95, 6932.61), time = 10.62.\n",
            "Batch 60: Hypervolume (random, qNEHVI) = (5133.95, 6932.61), time = 6.20.\n",
            "Batch 61: Hypervolume (random, qNEHVI) = (5133.95, 6932.61), time = 8.50.\n",
            "Batch 62: Hypervolume (random, qNEHVI) = (5133.95, 6932.61), time = 6.40.\n",
            "Batch 63: Hypervolume (random, qNEHVI) = (5133.95, 6932.61), time = 10.73.\n",
            "Batch 64: Hypervolume (random, qNEHVI) = (5133.95, 7030.33), time = 36.37.\n",
            "Batch 65: Hypervolume (random, qNEHVI) = (5133.95, 7030.33), time = 9.93.\n",
            "Batch 66: Hypervolume (random, qNEHVI) = (5133.95, 7030.33), time = 12.17.\n",
            "Batch 67: Hypervolume (random, qNEHVI) = (5133.95, 7030.33), time = 21.04.\n",
            "Batch 68: Hypervolume (random, qNEHVI) = (5133.95, 7030.33), time = 12.56.\n",
            "Batch 69: Hypervolume (random, qNEHVI) = (5133.95, 7030.33), time = 24.32.\n",
            "Batch 70: Hypervolume (random, qNEHVI) = (5133.95, 7030.33), time = 9.38."
          ]
        }
      ],
      "source": [
        "import time\n",
        "import warnings\n",
        "\n",
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.exceptions import BadInitialCandidatesWarning\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
        "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=BadInitialCandidatesWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "N_BATCH = 70\n",
        "MC_SAMPLES = 128 if not SMOKE_TEST else 16\n",
        "verbose = True\n",
        "\n",
        "hv = Hypervolume(ref_point=problem.ref_point)\n",
        "hvs_qnehvi, hvs_random = [], []\n",
        "\n",
        "# call helper functions to generate initial training data and initialize model\n",
        "train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi = generate_initial_data(\n",
        "    n=20\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "train_x_random, train_obj_random, train_con_random = (\n",
        "    train_x_qnehvi,\n",
        "    train_obj_qnehvi,\n",
        "    train_con_qnehvi,\n",
        ")\n",
        "\n",
        "mll_qnehvi, model_qnehvi = initialize_model(\n",
        "    train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi\n",
        ")\n",
        "\n",
        "# compute pareto front\n",
        "is_feas = (train_con_qnehvi <= 0).all(dim=-1)\n",
        "feas_train_obj = train_obj_qnehvi[is_feas]\n",
        "if feas_train_obj.shape[0] > 0:\n",
        "    pareto_mask = is_non_dominated(feas_train_obj)\n",
        "    pareto_y = feas_train_obj[pareto_mask]\n",
        "    # compute hypervolume\n",
        "    volume = hv.compute(pareto_y)\n",
        "else:\n",
        "    volume = 0.0\n",
        "\n",
        "hvs_qnehvi.append(volume)\n",
        "hvs_random.append(volume)\n",
        "\n",
        "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "for iteration in range(1, N_BATCH + 1):\n",
        "    t0 = time.monotonic()\n",
        "\n",
        "    # fit the models\n",
        "    fit_gpytorch_mll(mll_qnehvi)\n",
        "\n",
        "    # define the qParEGO and qNEHVI acquisition modules using a QMC sampler\n",
        "    qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
        "\n",
        "    # optimize acquisition functions and get new observations\n",
        "    new_x_qnehvi, new_obj_qnehvi, new_con_qnehvi = optimize_qnehvi_and_get_observation(\n",
        "        model_qnehvi, train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi, qnehvi_sampler\n",
        "    )\n",
        "    new_x_random, new_obj_random, new_con_random = generate_initial_data(n=BATCH_SIZE)\n",
        "\n",
        "    # update training points\n",
        "    train_x_qnehvi = torch.cat([train_x_qnehvi, new_x_qnehvi])\n",
        "    train_obj_qnehvi = torch.cat([train_obj_qnehvi, new_obj_qnehvi])\n",
        "    train_con_qnehvi = torch.cat([train_con_qnehvi, new_con_qnehvi])\n",
        "\n",
        "    train_x_random = torch.cat([train_x_random, new_x_random])\n",
        "    train_obj_random = torch.cat([train_obj_random, new_obj_random])\n",
        "    train_con_random = torch.cat([train_con_random, new_con_random])\n",
        "\n",
        "    # update progress\n",
        "    for hvs_list, train_obj, train_con in zip(\n",
        "        (hvs_random, hvs_qnehvi),\n",
        "        (train_obj_random, train_obj_qnehvi),\n",
        "        (train_con_random,  train_con_qnehvi),\n",
        "    ):\n",
        "        # compute pareto front\n",
        "        is_feas = (train_con <= 0).all(dim=-1)\n",
        "        feas_train_obj = train_obj[is_feas]\n",
        "        if feas_train_obj.shape[0] > 0:\n",
        "            pareto_mask = is_non_dominated(feas_train_obj)\n",
        "            pareto_y = feas_train_obj[pareto_mask]\n",
        "            # compute feasible hypervolume\n",
        "            volume = hv.compute(pareto_y)\n",
        "        else:\n",
        "            volume = 0.0\n",
        "        hvs_list.append(volume)\n",
        "\n",
        "    # reinitialize the models so they are ready for fitting on next iteration\n",
        "    # Note: we find improved performance from not warm starting the model hyperparameters\n",
        "    # using the hyperparameters from the previous iteration\n",
        "    mll_qnehvi, model_qnehvi = initialize_model(\n",
        "        train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi\n",
        "    )\n",
        "\n",
        "    t1 = time.monotonic()\n",
        "\n",
        "    if verbose:\n",
        "        print(\n",
        "            f\"\\nBatch {iteration:>2}: Hypervolume (random, qNEHVI) = \"\n",
        "            f\"({hvs_random[-1]:>4.2f}, {hvs_qnehvi[-1]:>4.2f}), \"\n",
        "            f\"time = {t1-t0:>4.2f}.\",\n",
        "            end=\"\",\n",
        "        )\n",
        "    else:\n",
        "        print(\".\", end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "3132af99-128f-41fc-9e6c-d1eaf6083f81",
        "showInput": false
      },
      "source": [
        "#### Plot the results\n",
        "The plot below shows the log feasible hypervolume difference: the log difference between the hypervolume of the true feasible pareto front and the hypervolume of the observed (feasible) pareto front identified by each algorithm. The log feasible hypervolume difference is plotted at each step of the optimization for each of the algorithms.\n",
        "\n",
        "The plot show that $q$NEHVI vastly outperforms the $q$ParEGO and Sobol baselines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651959825,
        "executionStopTime": 1668651960985,
        "hidden_ranges": [],
        "originalKey": "38f5ce01-264f-43bd-8bdb-edf756f7c0dc",
        "requestMsgId": "ec2a65b4-0cdb-4487-b6f4-da16df97ce18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diantong/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "Problem Penicillin does not specify maximal hypervolume.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/test_functions/base.py:187\u001b[0m, in \u001b[0;36mMultiObjectiveTestProblem.max_hv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_hv\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Penicillin' object has no attribute '_max_hv'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[94], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m iters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(N_BATCH \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m BATCH_SIZE\n\u001b[0;32m----> 9\u001b[0m log_hv_difference_qparego \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_hv\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(hvs_qparego))\n\u001b[1;32m     10\u001b[0m log_hv_difference_qnehvi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(problem\u001b[38;5;241m.\u001b[39mmax_hv \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(hvs_qnehvi))\n\u001b[1;32m     11\u001b[0m log_hv_difference_rnd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(problem\u001b[38;5;241m.\u001b[39mmax_hv \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(hvs_random))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/test_functions/base.py:189\u001b[0m, in \u001b[0;36mMultiObjectiveTestProblem.max_hv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_hv\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not specify maximal \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypervolume.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m     )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Problem Penicillin does not specify maximal hypervolume."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "iters = np.arange(N_BATCH + 1) * BATCH_SIZE\n",
        "log_hv_difference_qparego = np.log10(problem.max_hv - np.asarray(hvs_qparego))\n",
        "log_hv_difference_qnehvi = np.log10(problem.max_hv - np.asarray(hvs_qnehvi))\n",
        "log_hv_difference_rnd = np.log10(problem.max_hv - np.asarray(hvs_random))\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "ax.plot(\n",
        "    iters,\n",
        "    log_hv_difference_rnd,\n",
        "    label=\"Sobol\",\n",
        "    linewidth=1.5,\n",
        "    color=\"gray\",\n",
        ")\n",
        "ax.plot(\n",
        "    iters,\n",
        "    log_hv_difference_qparego,\n",
        "    label=\"qParEGO\",\n",
        "    linewidth=1.5,\n",
        "    color=\"red\",\n",
        ")\n",
        "ax.plot(\n",
        "    iters,\n",
        "    log_hv_difference_qnehvi,\n",
        "    label=\"qNEHVI\",\n",
        "    linewidth=1.5,\n",
        "    color=\"blue\",\n",
        ")\n",
        "ax.set(\n",
        "    xlabel=\"number of observations (beyond initial points)\",\n",
        "    ylabel=\"Log Hypervolume Difference\",\n",
        ")\n",
        "ax.legend(loc=\"lower right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "collapsed": true,
        "hidden_ranges": [],
        "originalKey": "a926c260-dcc2-4ce9-9c53-7b75456c6c0c",
        "showInput": false
      },
      "source": [
        "#### Plot the observations colored by iteration\n",
        "\n",
        "To examine optimization process from another perspective, we plot the collected observations under each algorithm where the color corresponds to the BO iteration at which the point was collected. The plot on the right for $q$NEHVI shows that the $q$NEHVI quickly identifies the pareto front and most of its evaluations are very close to the pareto front. $q$ParEGO also identifies has many observations close to the pareto front, but relies on optimizing random scalarizations, which is a less principled way of optimizing the pareto front compared to $q$NEHVI, which explicitly attempts focuses on improving the pareto front. Sobol generates random points and has few points close to the pareto front"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "customOutput": null,
        "executionStartTime": 1668651961290,
        "executionStopTime": 1668651961935,
        "originalKey": "75cb7f30-5ed2-4a02-bf46-b5c660af6494",
        "requestMsgId": "fa568aad-a7cb-43e7-b7ec-eff622a5edc9"
      },
      "outputs": [],
      "source": [
        "from matplotlib.cm import ScalarMappable\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
        "algos = [\"Sobol\", \"qParEGO\", \"qNEHVI\"]\n",
        "cm = plt.get_cmap(\"viridis\")\n",
        "\n",
        "batch_number = torch.cat(\n",
        "    [\n",
        "        torch.zeros(2 * (d + 1)),\n",
        "        torch.arange(1, N_BATCH + 1).repeat(BATCH_SIZE, 1).t().reshape(-1),\n",
        "    ]\n",
        ").numpy()\n",
        "\n",
        "for i, train_obj in enumerate((train_obj_random, train_obj_qparego, train_obj_qnehvi)):\n",
        "    sc = axes[i].scatter(\n",
        "        train_obj[:, 0].cpu().numpy(),\n",
        "        train_obj[:, 1].cpu().numpy(),\n",
        "        c=batch_number,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "    axes[i].set_title(algos[i])\n",
        "    axes[i].set_xlabel(\"Objective 1\")\n",
        "    axes[i].set_xlim(-2.5, 0)\n",
        "    axes[i].set_ylim(-2.5, 0)\n",
        "axes[0].set_ylabel(\"Objective 2\")\n",
        "norm = plt.Normalize(batch_number.min(), batch_number.max())\n",
        "sm = ScalarMappable(norm=norm, cmap=cm)\n",
        "sm.set_array([])\n",
        "fig.subplots_adjust(right=0.9)\n",
        "cbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.7])\n",
        "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
        "cbar.ax.set_title(\"Iteration\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
