{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "20bce0d3-6ea0-4db3-bc3b-2569027a0e3b",
        "showInput": false
      },
      "source": [
        "## Constrained, Parallel, Multi-Objective BO in BoTorch with qNEHVI, and qParEGO\n",
        "\n",
        "In this tutorial, we illustrate how to implement a constrained multi-objective (MO) Bayesian Optimization (BO) closed loop in BoTorch.\n",
        "\n",
        "In general, we recommend using [Ax](https://ax.dev) for a simple BO setup like this one, since this will simplify your setup (including the amount of code you need to write) considerably. See [here](https://ax.dev/tutorials/multiobjective_optimization.html) for an Ax tutorial on MOBO. If desired, you can use a custom BoTorch model in Ax, following the [Using BoTorch with Ax](./custom_botorch_model_in_ax) tutorial. Given a `MultiObjective`, Ax will default to the $q$NEHVI acquisiton function. If desired, this can also be customized by adding `\"botorch_acqf_class\": <desired_botorch_acquisition_function_class>,` to the `model_kwargs`.\n",
        "\n",
        "We use the parallel ParEGO ($q$ParEGO) [1] and parallel Noisy Expected Hypervolume Improvement ($q$NEHVI) [2]  acquisition functions to optimize a synthetic C2-DTLZ2 test function with $M=2$ objectives, $V=1$ constraint, and $d=4$ parameters. The two objectives are\n",
        "$$f_1(\\mathbf x) = (1+ g(\\mathbf x_M))\\cos\\big(\\frac{\\pi}{2}x_1\\big)$$\n",
        "$$f_2(\\mathbf x) = (1+ g(\\mathbf x_M))\\sin\\big(\\frac{\\pi}{2}x_1\\big)$$\n",
        "where $g(\\mathbf x) = \\sum_{x_i \\in \\mathbf x_M} (x_i - 0.5)^2, \\mathbf x \\in [0,1]^d,$ and $\\mathbf x_M$ represents the last $d - M +1$ elements of $\\mathbf x$. Additionally, the C2-DTLZ2 problem uses the following constraint:\n",
        "\n",
        "$$c(\\mathbf x) = - \\min \\bigg[\\min_{i=1}^M\\bigg((f_i(\\mathbf x) -1 )^2 + \\sum_{j=1, j=i}^M (f_j^2 - r^2) \\bigg), \\bigg(\\sum_{i=1}^M \\big((f_i(\\mathbf x) - \\frac{1}{\\sqrt{M}})^2 - r^2\\big)\\bigg)\\bigg]\\geq 0$$\n",
        "\n",
        "where $\\mathbf x \\in [0,1]^d$ and $r=0.2$. \n",
        "\n",
        "The goal here is to *minimize* both objectives. Since BoTorch assumes maximization, we maximize the negative of each objective. Since there typically is no single best solution in multi-objective optimization problems, we seek to find the pareto frontier, the set of optimal trade-offs where improving one metric means deteriorating another.\n",
        "\n",
        "[1] [S. Daulton, M. Balandat, and E. Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. Advances in Neural Information Processing Systems 33, 2020.](https://arxiv.org/abs/2006.05078)\n",
        "\n",
        "[2] [S. Daulton, M. Balandat, and E. Bakshy. Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement. Advances in Neural Information Processing Systems 34, 2021.](https://arxiv.org/abs/2105.08195)\n",
        "\n",
        "**For batch optimization (or in noisy settings), we strongly recommend using $q$NEHVI rather than $q$EHVI [1] because it is far more efficient than $q$EHVI and mathematically equivalent in the noiseless setting.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "00d9f0ef-fab6-463e-a54d-a548581bc7f4",
        "showInput": false
      },
      "source": [
        "### Set dtype and device\n",
        "Note: $q$EHVI aggressively exploits parallel hardware and is much faster when run on a GPU. See [1] for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "customOutput": null,
        "executionStartTime": 1668651350300,
        "executionStopTime": 1668651350308,
        "originalKey": "f27224aa-b567-4a6d-b6b3-74f2ecbfe319",
        "requestMsgId": "df1b7814-2d71-4421-b832-e10d0c1e7743"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "tkwargs = {\n",
        "    \"dtype\": torch.double,\n",
        "    \"device\": torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\"),\n",
        "}\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "89f8b99f-5cb2-45c9-9df6-7e1d18d4f8c6",
        "showInput": false
      },
      "source": [
        "### Problem setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "customOutput": null,
        "executionStartTime": 1668651350608,
        "executionStopTime": 1668651354486,
        "originalKey": "4227f250-60b5-4c97-b04c-3cfe7a1c410a",
        "requestMsgId": "83e67907-72c3-4bb8-8468-7eb99e616730"
      },
      "outputs": [],
      "source": [
        "from botorch.utils.sampling import draw_sobol_samples\n",
        "from botorch.test_functions.multi_objective import BraninCurrin\n",
        "problem = BraninCurrin(negate = True)\n",
        "problem.ref_point = torch.tensor([-20, -6]).to(**tkwargs)\n",
        "d = 2\n",
        "M = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "hidden_ranges": [],
        "originalKey": "2de9fbab-9d15-4410-8371-d3b1f730e3d7",
        "showInput": false
      },
      "source": [
        "#### Model initialization\n",
        "\n",
        "We use a multi-output `SingleTaskGP` to model the two objectives with a homoskedastic Gaussian likelihood with an inferred noise level.\n",
        "\n",
        "The models are initialized with $2(d+1)=10$ points drawn randomly from $[0,1]^{4}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651354720,
        "executionStopTime": 1668651354729,
        "hidden_ranges": [],
        "originalKey": "192b8d87-b2e3-4223-b193-6399b8643391",
        "requestMsgId": "55d97599-5be9-4a7a-857c-18a9b56bf07d"
      },
      "outputs": [],
      "source": [
        "from botorch.models.gp_regression import SingleTaskGP\n",
        "from botorch.models.model_list_gp_regression import ModelListGP\n",
        "from botorch.models.transforms.outcome import Standardize\n",
        "from botorch.utils.sampling import draw_sobol_samples\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
        "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
        "def evaluate_slack(X, ref = torch.tensor([-20, -6]).to(**tkwargs)):\n",
        "    Y = problem(X)\n",
        "    vio_raw = Y -ref\n",
        "    return (vio_raw).sum(dim = -1, keepdim = True)\n",
        "\n",
        "def generate_initial_data(n):\n",
        "    # generate training data\n",
        "    train_x = draw_sobol_samples(bounds=problem.bounds, n=n, q=1).squeeze(1)\n",
        "    train_obj = problem(train_x)\n",
        "    # negative values imply feasibility in botorch\n",
        "    train_con = -evaluate_slack(train_x)\n",
        "    return train_x, train_obj, train_con\n",
        "\n",
        "base = RBFKernel()\n",
        "covar_module = ScaleKernel(\n",
        "base_kernel=base,\n",
        ")\n",
        "def initialize_model(train_x, train_obj, train_con):\n",
        "    # define models for objective and constraint\n",
        "    train_x = normalize(train_x, problem.bounds)\n",
        "    train_y = torch.cat([train_obj, train_con], dim=-1)\n",
        "    models = []\n",
        "    for i in range(train_y.shape[-1]):\n",
        "        models.append(\n",
        "            SingleTaskGP(\n",
        "                train_x, train_y[..., i : i + 1], outcome_transform=Standardize(m=1),train_Yvar= torch.zeros((train_x.shape[0],1)) + 0.01**2,covar_module = covar_module)\n",
        "            )\n",
        "        \n",
        "    model = ModelListGP(*models)\n",
        "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
        "    return mll, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "hidden_ranges": [],
        "originalKey": "a0f6fa02-0843-4c45-8a19-8654226c1edc",
        "showInput": false
      },
      "source": [
        "#### Define a helper function that performs the essential BO step for $q$NEHVI\n",
        "The helper function below initializes the $q$NEHVI acquisition function, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. \n",
        "\n",
        "For this example, we'll use a small batch of $q=2$. Passing the keyword argument `sequential=True` to the function `optimize_acqf`specifies that candidates should be optimized in a sequential greedy fashion (see [1] for details why this is important). A simple initialization heuristic is used to select the 10 restart initial locations from a set of 512 random points. Multi-start optimization of the acquisition function is performed using LBFGS-B with exact gradients computed via auto-differentiation.\n",
        "\n",
        "**Reference Point**\n",
        "\n",
        "$q$NEHVI requires specifying a reference point, which is the lower bound on the objectives used for computing hypervolume. In this tutorial, we assume the reference point is known. In practice the reference point can be set 1) using domain knowledge to be slightly worse than the lower bound of objective values, where the lower bound is the minimum acceptable value of interest for each objective, or 2) using a dynamic reference point selection strategy.\n",
        "\n",
        "**Integrating over function values at in-sample designs**\n",
        "\n",
        "$q$NEHVI integrates over the unknown function values at the previously evaluated designs (see [2] for details). Therefore, we need to provide the previously evaluated designs (`train_x`, *normalized* to be within $[0,1]^d$) to the acquisition function.\n",
        "\n",
        "**Pruning baseline designs**\n",
        "To speed up integration over the function values at the previously evaluated designs, we prune the set of previously evaluated designs (by setting `prune_baseline=True`) to only include those which have positive probability of being on the current in-sample Pareto frontier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651354970,
        "executionStopTime": 1668651355060,
        "hidden_ranges": [],
        "originalKey": "65dcfbb2-f1e9-40a1-9807-8cdc1cc3fdc8",
        "requestMsgId": "68a072df-7e90-4c7f-9915-520ca48c5e0a"
      },
      "outputs": [],
      "source": [
        "from botorch.acquisition.multi_objective.monte_carlo import (\n",
        "    qNoisyExpectedHypervolumeImprovement,\n",
        ")\n",
        "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n",
        "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
        "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
        "from botorch.utils.sampling import sample_simplex\n",
        "\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
        "RAW_SAMPLES = 512 if not SMOKE_TEST else 4\n",
        "\n",
        "standard_bounds = torch.zeros(2, problem.dim, **tkwargs)\n",
        "standard_bounds[1] = 1\n",
        "\n",
        "\n",
        "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, train_con, sampler):\n",
        "    \"\"\"Optimizes the qNEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
        "    train_x = normalize(train_x, problem.bounds)\n",
        "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
        "        model=model,\n",
        "        ref_point= problem.ref_point.tolist(),  # use known reference point\n",
        "        X_baseline=train_x,\n",
        "        sampler=sampler,\n",
        "        prune_baseline=False,\n",
        "        # define an objective that specifies which outcomes are the objectives\n",
        "        objective=IdentityMCMultiOutputObjective(outcomes=[0, 1]),\n",
        "        # specify that the constraint is on the last outcome\n",
        "        constraints=[lambda Z: Z[..., -1]],\n",
        "    )\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=standard_bounds,\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
        "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
        "        sequential=True,\n",
        "    )\n",
        "    # observe new values\n",
        "    new_x = unnormalize(candidates.detach(), bounds=problem.bounds)\n",
        "    new_obj = problem(new_x)\n",
        "    # negative values imply feasibility in botorch\n",
        "    new_con = -evaluate_slack(new_x)\n",
        "    return new_x, new_obj, new_con"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "d4487ba0-4fad-41dd-a2ae-e0b95094dba1",
        "showInput": false
      },
      "source": [
        "### Perform Bayesian Optimization loop with $q$EHVI and $q$ParEGO\n",
        "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps:\n",
        "1. given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$\n",
        "2. observe $f(x)$ for each $x$ in the batch \n",
        "3. update the surrogate model. \n",
        "\n",
        "\n",
        "Just for illustration purposes, we run one trial with `N_BATCH=20` rounds of optimization. The acquisition function is approximated using `MC_SAMPLES=128` samples.\n",
        "\n",
        "*Note*: Running this may take a little while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def voxel_grid_sampling_with_indices(points, voxel_size = 5.0):\n",
        "    # Calculate the minimum and maximum coordinates\n",
        "    min_coords = torch.min(points, dim=0).values\n",
        "    max_coords = torch.max(points, dim=0).values\n",
        "\n",
        "    # Shift points so that the minimum coordinates are at the origin\n",
        "    shifted_points = points - min_coords\n",
        "\n",
        "    # Quantize the points to voxel grid coordinates\n",
        "    voxel_indices = torch.floor(shifted_points / voxel_size).long()\n",
        "\n",
        "    # Use a dictionary to store unique voxel indices and the corresponding row index\n",
        "    voxel_dict = {}\n",
        "    for idx, voxel_idx in enumerate(voxel_indices):\n",
        "        voxel_idx_tuple = tuple(voxel_idx.tolist())\n",
        "        if voxel_idx_tuple not in voxel_dict:\n",
        "            voxel_dict[voxel_idx_tuple] = idx\n",
        "\n",
        "    # Extract the row indices of the sampled points\n",
        "    sampled_indices = torch.tensor(list(voxel_dict.values()))\n",
        "\n",
        "    return sampled_indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651356028,
        "executionStopTime": 1668651959470,
        "hidden_ranges": [],
        "originalKey": "4c225d99-6425-4201-ac4a-a042a351c1d3",
        "requestMsgId": "be831f3c-ff7c-4c00-a215-fb021f0c5770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch  1: Hypervolume (random, qNEHVI) = (7.86, 37.42), time = 12.25.\n",
            "Batch  2: Hypervolume (random, qNEHVI) = (7.86, 37.42), time = 3.46.\n",
            "Batch  3: Hypervolume (random, qNEHVI) = (7.86, 48.37), time = 1.14.\n",
            "Batch  4: Hypervolume (random, qNEHVI) = (7.86, 55.82), time = 5.11.\n",
            "Batch  5: Hypervolume (random, qNEHVI) = (7.86, 56.12), time = 2.62.\n",
            "Batch  6: Hypervolume (random, qNEHVI) = (7.86, 57.86), time = 5.08.\n",
            "Batch  7: Hypervolume (random, qNEHVI) = (7.86, 59.72), time = 11.50.\n",
            "Batch  8: Hypervolume (random, qNEHVI) = (7.86, 59.72), time = 1.87.\n",
            "Batch  9: Hypervolume (random, qNEHVI) = (7.86, 61.44), time = 2.34.\n",
            "Batch 10: Hypervolume (random, qNEHVI) = (7.86, 61.44), time = 1.21.\n",
            "Batch 11: Hypervolume (random, qNEHVI) = (7.86, 63.05), time = 1.50.\n",
            "Batch 12: Hypervolume (random, qNEHVI) = (7.86, 63.88), time = 1.78."
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m qnehvi_sampler \u001b[38;5;241m=\u001b[39m SobolQMCNormalSampler(sample_shape\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mSize([MC_SAMPLES]))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# optimize acquisition functions and get new observations\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m new_x_qnehvi, new_obj_qnehvi, new_con_qnehvi \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_qnehvi_and_get_observation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_qnehvi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_x_qnehvi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_obj_qnehvi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_con_qnehvi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnehvi_sampler\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m new_x_random, new_obj_random, new_con_random \u001b[38;5;241m=\u001b[39m generate_initial_data(n\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# update training points\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[10], line 33\u001b[0m, in \u001b[0;36moptimize_qnehvi_and_get_observation\u001b[0;34m(model, train_x, train_obj, train_con, sampler)\u001b[0m\n\u001b[1;32m     21\u001b[0m acq_func \u001b[38;5;241m=\u001b[39m qNoisyExpectedHypervolumeImprovement(\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     23\u001b[0m     ref_point\u001b[38;5;241m=\u001b[39m problem\u001b[38;5;241m.\u001b[39mref_point\u001b[38;5;241m.\u001b[39mtolist(),  \u001b[38;5;66;03m# use known reference point\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     constraints\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mlambda\u001b[39;00m Z: Z[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]],\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# optimize\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m candidates, _ \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_acqf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43macq_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstandard_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_RESTARTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRAW_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# used for intialization heuristic\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_limit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# observe new values\u001b[39;00m\n\u001b[1;32m     43\u001b[0m new_x \u001b[38;5;241m=\u001b[39m unnormalize(candidates\u001b[38;5;241m.\u001b[39mdetach(), bounds\u001b[38;5;241m=\u001b[39mproblem\u001b[38;5;241m.\u001b[39mbounds)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/optim/optimize.py:567\u001b[0m, in \u001b[0;36moptimize_acqf\u001b[0;34m(acq_function, bounds, q, num_restarts, raw_samples, options, inequality_constraints, equality_constraints, nonlinear_inequality_constraints, fixed_features, post_processing_func, batch_initial_conditions, return_best_only, gen_candidates, sequential, ic_generator, timeout_sec, return_full_tree, retry_on_optimization_warning, **ic_gen_kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m     gen_candidates \u001b[38;5;241m=\u001b[39m gen_candidates_scipy\n\u001b[1;32m    545\u001b[0m opt_acqf_inputs \u001b[38;5;241m=\u001b[39m OptimizeAcqfInputs(\n\u001b[1;32m    546\u001b[0m     acq_function\u001b[38;5;241m=\u001b[39macq_function,\n\u001b[1;32m    547\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m     ic_gen_kwargs\u001b[38;5;241m=\u001b[39mic_gen_kwargs,\n\u001b[1;32m    566\u001b[0m )\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_acqf_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/optim/optimize.py:588\u001b[0m, in \u001b[0;36m_optimize_acqf\u001b[0;34m(opt_inputs)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _optimize_acqf_sequential_q(opt_inputs\u001b[38;5;241m=\u001b[39mopt_inputs)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;66;03m# Batch optimization (including the case q=1)\u001b[39;00m\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize_acqf_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/optim/optimize.py:386\u001b[0m, in \u001b[0;36m_optimize_acqf_batch\u001b[0;34m(opt_inputs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initial_conditions_provided:\n\u001b[1;32m    373\u001b[0m     batch_initial_conditions \u001b[38;5;241m=\u001b[39m opt_inputs\u001b[38;5;241m.\u001b[39mget_ic_generator()(\n\u001b[1;32m    374\u001b[0m         acq_function\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39macq_function,\n\u001b[1;32m    375\u001b[0m         bounds\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39mbounds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39mic_gen_kwargs,\n\u001b[1;32m    384\u001b[0m     )\n\u001b[0;32m--> 386\u001b[0m     batch_candidates, batch_acq_values, ws \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_batch_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     optimization_warning_raised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    389\u001b[0m         (\u001b[38;5;28missubclass\u001b[39m(w\u001b[38;5;241m.\u001b[39mcategory, OptimizationWarning) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m ws)\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m optimization_warning_raised:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/optim/optimize.py:336\u001b[0m, in \u001b[0;36m_optimize_acqf_batch.<locals>._optimize_batch_candidates\u001b[0;34m()\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ws:\n\u001b[1;32m    332\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malways\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mOptimizationWarning)\n\u001b[1;32m    333\u001b[0m     (\n\u001b[1;32m    334\u001b[0m         batch_candidates_curr,\n\u001b[1;32m    335\u001b[0m         batch_acq_values_curr,\n\u001b[0;32m--> 336\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mopt_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_ics_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macq_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_gen_kwargs\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m opt_warnings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ws\n\u001b[1;32m    340\u001b[0m batch_candidates_list\u001b[38;5;241m.\u001b[39mappend(batch_candidates_curr)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/generation/gen.py:252\u001b[0m, in \u001b[0;36mgen_candidates_scipy\u001b[0;34m(initial_conditions, acquisition_function, lower_bounds, upper_bounds, inequality_constraints, equality_constraints, nonlinear_inequality_constraints, options, fixed_features, timeout_sec)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39macquisition_function(x)\n\u001b[0;32m--> 252\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mminimize_with_timeout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf_np_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSLSQP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallback\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallback\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_grad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m _process_scipy_result(res\u001b[38;5;241m=\u001b[39mres, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    270\u001b[0m candidates \u001b[38;5;241m=\u001b[39m fix_features(\n\u001b[1;32m    271\u001b[0m     X\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(res\u001b[38;5;241m.\u001b[39mx)\u001b[38;5;241m.\u001b[39mto(initial_conditions)\u001b[38;5;241m.\u001b[39mreshape(shapeX),\n\u001b[1;32m    272\u001b[0m     fixed_features\u001b[38;5;241m=\u001b[39mfixed_features,\n\u001b[1;32m    273\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/optim/utils/timeout.py:82\u001b[0m, in \u001b[0;36mminimize_with_timeout\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options, timeout_sec)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod .* cannot handle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhessp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrapped_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OptimizationTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     97\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization timed out after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/optimize/_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:369\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    363\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/optimize/_optimize.py:78\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/optimize/_optimize.py:72\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 72\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/generation/gen.py:209\u001b[0m, in \u001b[0;36mgen_candidates_scipy.<locals>.f_np_wrapper\u001b[0;34m(x, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m X \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    202\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;241m.\u001b[39mto(initial_conditions)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    207\u001b[0m )\n\u001b[1;32m    208\u001b[0m X_fix \u001b[38;5;241m=\u001b[39m fix_features(X, fixed_features\u001b[38;5;241m=\u001b[39mfixed_features)\n\u001b[0;32m--> 209\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fix\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# compute gradient w.r.t. the inputs (does not accumulate in leaves)\u001b[39;00m\n\u001b[1;32m    211\u001b[0m gradf \u001b[38;5;241m=\u001b[39m _arrayify(torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss, X)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/generation/gen.py:250\u001b[0m, in \u001b[0;36mgen_candidates_scipy.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[43macquisition_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/utils/transforms.py:305\u001b[0m, in \u001b[0;36mconcatenate_pending_points.<locals>.decorated\u001b[0;34m(cls, X, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mX_pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([X, match_batch_shape(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mX_pending, X)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/utils/transforms.py:259\u001b[0m, in \u001b[0;36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[0;34m(acqf, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# add t-batch dim\u001b[39;00m\n\u001b[1;32m    258\u001b[0m X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 259\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43macqf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(acqf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_ensemble(acqf\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# IDEA: this could be wrapped into SampleReducingMCAcquisitionFunction\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    263\u001b[0m         output\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m acqf\u001b[38;5;241m.\u001b[39m_log \u001b[38;5;28;01melse\u001b[39;00m logmeanexp(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    264\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/acquisition/multi_objective/monte_carlo.py:453\u001b[0m, in \u001b[0;36mqNoisyExpectedHypervolumeImprovement.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    442\u001b[0m X_full \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([match_batch_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_baseline, X), X], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# NOTE: To ensure that we correctly sample `f(X)` from the joint distribution\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# `f((X_baseline, X)) ~ P(f | D)`, it is critical to compute the joint posterior\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# over X *and* X_baseline -- which also contains pending points whenever there\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# 1) X and X, and\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# 2) X and X_baseline.\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m posterior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Account for possible one-to-many transform and the MCMC batch dimension in\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# `SaasFullyBayesianSingleTaskGP`\u001b[39;00m\n\u001b[1;32m    456\u001b[0m event_shape_lag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_ensemble(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m2\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/models/gpytorch.py:659\u001b[0m, in \u001b[0;36mModelListGPyTorchModel.posterior\u001b[0;34m(self, X, output_indices, observation_noise, posterior_transform, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m returns_untransformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcome_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m mod\u001b[38;5;241m.\u001b[39moutcome_transform\u001b[38;5;241m.\u001b[39m_is_linear)\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\n\u001b[1;32m    657\u001b[0m )\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# NOTE: We're not passing in the posterior transform here. We'll apply it later.\u001b[39;00m\n\u001b[0;32m--> 659\u001b[0m posterior \u001b[38;5;241m=\u001b[39m \u001b[43mModelList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m returns_untransformed:\n\u001b[1;32m    667\u001b[0m     mvns \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m posterior\u001b[38;5;241m.\u001b[39mposteriors]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/models/model.py:519\u001b[0m, in \u001b[0;36mModelList.posterior\u001b[0;34m(self, X, output_indices, observation_noise, posterior_transform, **kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m         obs_noise \u001b[38;5;241m=\u001b[39m observation_noise\n\u001b[1;32m    518\u001b[0m     posteriors\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 519\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_noise\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m posterior \u001b[38;5;241m=\u001b[39m PosteriorList(\u001b[38;5;241m*\u001b[39mposteriors)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m posterior_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/models/gpytorch.py:395\u001b[0m, in \u001b[0;36mBatchedMultiOutputGPyTorchModel.posterior\u001b[0;34m(self, X, output_indices, observation_noise, posterior_transform, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     X, output_dim_idx \u001b[38;5;241m=\u001b[39m add_output_dim(\n\u001b[1;32m    390\u001b[0m         X\u001b[38;5;241m=\u001b[39mX, original_batch_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_batch_shape\n\u001b[1;32m    391\u001b[0m     )\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# NOTE: BoTorch's GPyTorchModels also inherit from GPyTorch's ExactGP, thus\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# self(X) calls GPyTorch's ExactGP's __call__, which computes the posterior,\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# rather than e.g. SingleTaskGP's forward, which computes the prior.\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m mvn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation_noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:316\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     full_inputs\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mcat([train_input, \u001b[38;5;28minput\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Get the joint distribution for training/test data\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mExactGP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mdebug()\u001b[38;5;241m.\u001b[39mon():\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(full_output, MultivariateNormal):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/models/gp_regression.py:240\u001b[0m, in \u001b[0;36mSingleTaskGP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    239\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_inputs(x)\n\u001b[0;32m--> 240\u001b[0m mean_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m covar_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_module(x)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MultivariateNormal(mean_x, covar_x)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gpytorch/means/mean.py:22\u001b[0m, in \u001b[0;36mMean.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gpytorch/means/constant_mean.py:111\u001b[0m, in \u001b[0;36mConstantMean.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    110\u001b[0m     constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstant\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# *batch_shape x 1\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconstant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "import warnings\n",
        "\n",
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.exceptions import BadInitialCandidatesWarning\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
        "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=BadInitialCandidatesWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "random_seeds = [83810, 14592, 3278, 97196, 36048, 32098, 29256, 18289, 96530, 13434, 88696, 97080, 71482, 11395, 77397, 55302, 4165, 3905, 12280, 28657, 30495, 66237, 78907, 3478, 73563,\n",
        "26062, 93850, 85181, 91924, 71426, 54987, 28893, 58878, 77236, 36463, 851, 99458, 20926, 91506, 55392, 44597, 36421, 20379, 28221, 44118, 13396, 12156, 49797, 12676, 47052]\n",
        "declared = False\n",
        "\n",
        "N_BATCH = 100\n",
        "MC_SAMPLES = 128 if not SMOKE_TEST else 16\n",
        "verbose = True\n",
        "c = 0\n",
        "for seed in random_seeds[:10]:\n",
        "    torch.manual_seed(seed)\n",
        "    train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi = generate_initial_data(10)\n",
        "    hv = Hypervolume(ref_point=problem.ref_point)\n",
        "    hvs_qnehvi, hvs_random = [], []\n",
        "    # call helper functions to generate initial training data and initialize model\n",
        "\n",
        "\n",
        "\n",
        "    train_x_random, train_obj_random, train_con_random = (\n",
        "        train_x_qnehvi,\n",
        "        train_obj_qnehvi,\n",
        "        train_con_qnehvi,\n",
        "    )\n",
        "\n",
        "    mll_qnehvi, model_qnehvi = initialize_model(\n",
        "        train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi\n",
        "    )\n",
        "\n",
        "    # compute pareto front\n",
        "    is_feas = (train_con_qnehvi <= 0).all(dim=-1)\n",
        "    feas_train_obj = train_obj_qnehvi[is_feas]\n",
        "    if feas_train_obj.shape[0] > 0:\n",
        "        pareto_mask = is_non_dominated(feas_train_obj)\n",
        "        pareto_y = feas_train_obj[pareto_mask]\n",
        "        # compute hypervolume\n",
        "        volume = hv.compute(pareto_y)\n",
        "    else:\n",
        "        volume = 0.0\n",
        "\n",
        "    hvs_qnehvi.append(volume)\n",
        "    hvs_random.append(volume)\n",
        "    # run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "    for iteration in range(1, N_BATCH + 1):\n",
        "        t0 = time.monotonic()\n",
        "\n",
        "        # fit the models\n",
        "        fit_gpytorch_mll(mll_qnehvi)\n",
        "\n",
        "        # define the qParEGO and qNEHVI acquisition modules using a QMC sampler\n",
        "        qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
        "\n",
        "        # optimize acquisition functions and get new observations\n",
        "        new_x_qnehvi, new_obj_qnehvi, new_con_qnehvi = optimize_qnehvi_and_get_observation(\n",
        "            model_qnehvi, train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi, qnehvi_sampler\n",
        "        )\n",
        "        new_x_random, new_obj_random, new_con_random = generate_initial_data(n=BATCH_SIZE)\n",
        "\n",
        "        # update training points\n",
        "        train_x_qnehvi = torch.cat([train_x_qnehvi, new_x_qnehvi])\n",
        "        train_obj_qnehvi = torch.cat([train_obj_qnehvi, new_obj_qnehvi])\n",
        "        train_con_qnehvi = torch.cat([train_con_qnehvi, new_con_qnehvi])\n",
        "\n",
        "        train_x_random = torch.cat([train_x_random, new_x_random])\n",
        "        train_obj_random = torch.cat([train_obj_random, new_obj_random])\n",
        "        train_con_random = torch.cat([train_con_random, new_con_random])\n",
        "\n",
        "        # update progress\n",
        "        for hvs_list, train_obj, train_con in zip(\n",
        "            (hvs_random, hvs_qnehvi),\n",
        "            (train_obj_random, train_obj_qnehvi),\n",
        "            (train_con_random,  train_con_qnehvi),\n",
        "        ):\n",
        "            # compute pareto front\n",
        "            is_feas = (train_con <= 0).all(dim=-1)\n",
        "            feas_train_obj = train_obj[is_feas]\n",
        "            if feas_train_obj.shape[0] > 0:\n",
        "                pareto_mask = is_non_dominated(feas_train_obj)\n",
        "                pareto_y = feas_train_obj[pareto_mask]\n",
        "                # compute feasible hypervolume\n",
        "                volume = hv.compute(pareto_y)\n",
        "            else:\n",
        "                volume = 0.0\n",
        "            hvs_list.append(volume)\n",
        "\n",
        "        # reinitialize the models so they are ready for fitting on next iteration\n",
        "        # Note: we find improved performance from not warm starting the model hyperparameters\n",
        "        # using the hyperparameters from the previous iteration\n",
        "        mll_qnehvi, model_qnehvi = initialize_model(\n",
        "            train_x_qnehvi, train_obj_qnehvi, train_con_qnehvi\n",
        "        )\n",
        "\n",
        "        t1 = time.monotonic()\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"\\nBatch {iteration:>2}: Hypervolume (random, qNEHVI) = \"\n",
        "                f\"({hvs_random[-1]:>4.2f}, {hvs_qnehvi[-1]:>4.2f}), \"\n",
        "                f\"time = {t1-t0:>4.2f}.\",\n",
        "                end=\"\",\n",
        "            )\n",
        "        else:\n",
        "            print(\".\", end=\"\")\n",
        "    c+=1\n",
        "    torch.save(torch.tensor(hvs_qnehvi), f'hv_bar_ehvi_{c}.pt')\n",
        "    torch.save(train_obj_qnehvi, f'obj_bar_ehvi_{c}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "3132af99-128f-41fc-9e6c-d1eaf6083f81",
        "showInput": false
      },
      "source": [
        "#### Plot the results\n",
        "The plot below shows the log feasible hypervolume difference: the log difference between the hypervolume of the true feasible pareto front and the hypervolume of the observed (feasible) pareto front identified by each algorithm. The log feasible hypervolume difference is plotted at each step of the optimization for each of the algorithms.\n",
        "\n",
        "The plot show that $q$NEHVI vastly outperforms the $q$ParEGO and Sobol baselines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [],
        "customOutput": null,
        "executionStartTime": 1668651959825,
        "executionStopTime": 1668651960985,
        "hidden_ranges": [],
        "originalKey": "38f5ce01-264f-43bd-8bdb-edf756f7c0dc",
        "requestMsgId": "ec2a65b4-0cdb-4487-b6f4-da16df97ce18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/diantong/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "Problem Penicillin does not specify maximal hypervolume.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/test_functions/base.py:187\u001b[0m, in \u001b[0;36mMultiObjectiveTestProblem.max_hv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_hv\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Penicillin' object has no attribute '_max_hv'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m iters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(N_BATCH \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m BATCH_SIZE\n\u001b[0;32m----> 9\u001b[0m log_hv_difference_qparego \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_hv\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(hvs_qparego))\n\u001b[1;32m     10\u001b[0m log_hv_difference_qnehvi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(problem\u001b[38;5;241m.\u001b[39mmax_hv \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(hvs_qnehvi))\n\u001b[1;32m     11\u001b[0m log_hv_difference_rnd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(problem\u001b[38;5;241m.\u001b[39mmax_hv \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(hvs_random))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botorch/test_functions/base.py:189\u001b[0m, in \u001b[0;36mMultiObjectiveTestProblem.max_hv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_hv\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not specify maximal \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypervolume.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m     )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Problem Penicillin does not specify maximal hypervolume."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "iters = np.arange(N_BATCH + 1) * BATCH_SIZE\n",
        "log_hv_difference_qparego = np.log10(problem.max_hv - np.asarray(hvs_qparego))\n",
        "log_hv_difference_qnehvi = np.log10(problem.max_hv - np.asarray(hvs_qnehvi))\n",
        "log_hv_difference_rnd = np.log10(problem.max_hv - np.asarray(hvs_random))\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "ax.plot(\n",
        "    iters,\n",
        "    log_hv_difference_rnd,\n",
        "    label=\"Sobol\",\n",
        "    linewidth=1.5,\n",
        "    color=\"gray\",\n",
        ")\n",
        "ax.plot(\n",
        "    iters,\n",
        "    log_hv_difference_qparego,\n",
        "    label=\"qParEGO\",\n",
        "    linewidth=1.5,\n",
        "    color=\"red\",\n",
        ")\n",
        "ax.plot(\n",
        "    iters,\n",
        "    log_hv_difference_qnehvi,\n",
        "    label=\"qNEHVI\",\n",
        "    linewidth=1.5,\n",
        "    color=\"blue\",\n",
        ")\n",
        "ax.set(\n",
        "    xlabel=\"number of observations (beyond initial points)\",\n",
        "    ylabel=\"Log Hypervolume Difference\",\n",
        ")\n",
        "ax.legend(loc=\"lower right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "collapsed": true,
        "hidden_ranges": [],
        "originalKey": "a926c260-dcc2-4ce9-9c53-7b75456c6c0c",
        "showInput": false
      },
      "source": [
        "#### Plot the observations colored by iteration\n",
        "\n",
        "To examine optimization process from another perspective, we plot the collected observations under each algorithm where the color corresponds to the BO iteration at which the point was collected. The plot on the right for $q$NEHVI shows that the $q$NEHVI quickly identifies the pareto front and most of its evaluations are very close to the pareto front. $q$ParEGO also identifies has many observations close to the pareto front, but relies on optimizing random scalarizations, which is a less principled way of optimizing the pareto front compared to $q$NEHVI, which explicitly attempts focuses on improving the pareto front. Sobol generates random points and has few points close to the pareto front"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "customOutput": null,
        "executionStartTime": 1668651961290,
        "executionStopTime": 1668651961935,
        "originalKey": "75cb7f30-5ed2-4a02-bf46-b5c660af6494",
        "requestMsgId": "fa568aad-a7cb-43e7-b7ec-eff622a5edc9"
      },
      "outputs": [],
      "source": [
        "from matplotlib.cm import ScalarMappable\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
        "algos = [\"Sobol\", \"qParEGO\", \"qNEHVI\"]\n",
        "cm = plt.get_cmap(\"viridis\")\n",
        "\n",
        "batch_number = torch.cat(\n",
        "    [\n",
        "        torch.zeros(2 * (d + 1)),\n",
        "        torch.arange(1, N_BATCH + 1).repeat(BATCH_SIZE, 1).t().reshape(-1),\n",
        "    ]\n",
        ").numpy()\n",
        "\n",
        "for i, train_obj in enumerate((train_obj_random, train_obj_qparego, train_obj_qnehvi)):\n",
        "    sc = axes[i].scatter(\n",
        "        train_obj[:, 0].cpu().numpy(),\n",
        "        train_obj[:, 1].cpu().numpy(),\n",
        "        c=batch_number,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "    axes[i].set_title(algos[i])\n",
        "    axes[i].set_xlabel(\"Objective 1\")\n",
        "    axes[i].set_xlim(-2.5, 0)\n",
        "    axes[i].set_ylim(-2.5, 0)\n",
        "axes[0].set_ylabel(\"Objective 2\")\n",
        "norm = plt.Normalize(batch_number.min(), batch_number.max())\n",
        "sm = ScalarMappable(norm=norm, cmap=cm)\n",
        "sm.set_array([])\n",
        "fig.subplots_adjust(right=0.9)\n",
        "cbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.7])\n",
        "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
        "cbar.ax.set_title(\"Iteration\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
