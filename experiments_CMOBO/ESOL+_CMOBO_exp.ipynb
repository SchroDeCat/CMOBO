{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from botorch.utils import t_batch_mode_transform\n",
    "import torch\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "from botorch.acquisition.monte_carlo import MCAcquisitionFunction\n",
    "from botorch.acquisition.monte_carlo import AcquisitionFunction\n",
    "from botorch.optim.optimize import optimize_acqf_discrete\n",
    "from botorch.optim.initializers import gen_batch_initial_conditions\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "\n",
    "# kernels\n",
    "from gpytorch.kernels import RBFKernel, MaternKernel, LinearKernel, ScaleKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem setting: ESOL+ data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "\n",
    "target = torch.load(\"target_ESOL.pt\")\n",
    "domain = torch.load(\"domain_ESOL.pt\")\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def test_f(X: torch.Tensor, tensor: torch.Tensor) -> int:\n",
    "    # Compare the 1*d tensor (row) with each row in the n*d tensor\n",
    "    matches = (tensor == X).all(dim=1)\n",
    "\n",
    "    # Find the index of the matching row\n",
    "    match_idx = torch.where(matches)[0][-1]\n",
    "    # If a match is found, return the index\n",
    "\n",
    "    return match_idx.item()\n",
    "\n",
    "    # Stack the list of rows into a tensor and return it\n",
    "    output = torch.stack(output_rows)\n",
    "    return output\n",
    "\n",
    "\n",
    "def generate_initial_data(n):\n",
    "    # generate training data\n",
    "    ind = random.sample(range(target.shape[0]), n)\n",
    "    return ind\n",
    "\n",
    "\n",
    "ub = torch.max(domain, dim=0)[0]\n",
    "lb = torch.min(domain, dim=0)[0]\n",
    "\n",
    "d = domain.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "import torch\n",
    "\n",
    "\n",
    "class HyperVolumeScalarizedUCB(AnalyticAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        beta: float,\n",
    "        theta: torch.Tensor,\n",
    "        ref: torch.Tensor,\n",
    "        maximize: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the HyperVolume Scalarized Upper Confidence Bound Acquisition Function.\n",
    "\n",
    "        Args:\n",
    "            model: A BoTorch model representing the posterior distribution of the objectives.\n",
    "            beta (Tensor of shape [1] or [o]): The exploration-exploitation trade-off parameter(s).\n",
    "            theta (Tensor of shape [o]): The weights used for scalarizing the upper bounds, where `o` is the number of objectives.\n",
    "            maximize (bool): Whether to maximize or minimize the scalarized objective. Defaults to True (maximize).\n",
    "        \"\"\"\n",
    "        super(AnalyticAcquisitionFunction, self).__init__(model)\n",
    "        self.maximize = maximize\n",
    "        self.register_buffer(\"beta\", torch.as_tensor(beta))\n",
    "        self.register_buffer(\"theta\", torch.as_tensor(theta))\n",
    "        self.register_buffer(\"ref\", torch.as_tensor(ref))\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluate the scalarized Upper Confidence Bound on the candidate set X.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor of shape [b, d]): A tensor containing `(b)` batches of `d`-dimensional design points.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [b]: A tensor containing the scalarized Upper Confidence Bound values for each batch.\n",
    "        \"\"\"\n",
    "        self.beta = self.beta.to(X)\n",
    "        self.theta = self.theta.to(X)\n",
    "        self.ref = self.ref.to(X)\n",
    "        posterior = self.model.posterior(X)\n",
    "        means = posterior.mean.squeeze(dim=-2)  # b x o\n",
    "        std_devs = posterior.variance.squeeze(dim=-2).sqrt()  # b x o\n",
    "        m = means.shape[1]\n",
    "        # Calculate upper confidence bounds for each objective\n",
    "        u_t = means + (self.beta.expand_as(means) * std_devs) - self.ref  # b x o\n",
    "\n",
    "        # Apply the scalarization function to the upper bounds\n",
    "        scalarized_ut = torch.min(\n",
    "            torch.max(torch.zeros_like(u_t), u_t / self.theta) ** m, dim=-1\n",
    "        )[\n",
    "            0\n",
    "        ]  # b\n",
    "\n",
    "        return scalarized_ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Acq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryAcq(MCAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        beta: float,\n",
    "        theta: torch.Tensor,\n",
    "        ref: torch.Tensor,\n",
    "        maximize: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        An auxiliary acquisition defined in Algo.2\n",
    "\n",
    "        Args:\n",
    "            model: A BoTorch model representing the posterior distribution of the objectives.\n",
    "            beta (Tensor of shape [1] or [o]): The exploration-exploitation trade-off parameter(s).\n",
    "            theta (Tensor of shape [o]): The weights used for scalarizing the upper bounds, where `o` is the number of objectives.\n",
    "            maximize (bool): Whether to maximize or minimize the scalarized objective. Defaults to True (maximize).\n",
    "        \"\"\"\n",
    "        super(MCAcquisitionFunction, self).__init__(model)\n",
    "        self.maximize = maximize\n",
    "        self.register_buffer(\"beta\", torch.as_tensor(beta))\n",
    "        self.register_buffer(\"theta\", torch.as_tensor(theta))\n",
    "        self.register_buffer(\"ref\", torch.as_tensor(ref))\n",
    "\n",
    "    @t_batch_mode_transform()\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluate the scalarized Upper Confidence Bound on the candidate set X.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor of shape [b, d]): A tensor containing `(b)` batches of `d`-dimensional design points.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [b]: A tensor containing the scalarized Upper Confidence Bound values for each batch.\n",
    "        \"\"\"\n",
    "        self.beta = self.beta.to(X)\n",
    "        self.theta = self.theta.to(X)\n",
    "        self.ref = self.ref.to(X)\n",
    "        posterior = self.model.posterior(X)\n",
    "        # print(posterior.mean.shape)\n",
    "        means = posterior.mean  # b x q x o\n",
    "        std_devs = posterior.variance.sqrt()  # b x q x o\n",
    "        # Calculate upper confidence bounds for each objective\n",
    "        u_t = means + (self.beta.expand_as(means) * std_devs) - self.ref  # b x qx o\n",
    "        # print('233', u_t.shape)\n",
    "\n",
    "        # Apply the scalarization function to the upper bounds\n",
    "        scalarized_ut = torch.min(torch.min(u_t, dim=-1)[0], dim=-1)[0]  # b\n",
    "        return scalarized_ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "\n",
    "def create_ucb_constraints(model, beta: float, thresholds: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Creates a list of non-linear inequality constraints for a multi-output GP model, ensuring that the upper confidence\n",
    "    bounds of the model's outputs are greater than or equal to the specified thresholds.\n",
    "\n",
    "    Args:\n",
    "        model (MultiTaskGP): A multi-output Gaussian Process model.\n",
    "        beta (float): The scalar coefficient for the variance component of the UCB.\n",
    "        thresholds (torch.Tensor): A tensor of thresholds for each output dimension.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Callable, bool]]: A list of tuples, each containing a callable constraint and a boolean indicating\n",
    "                                      whether the constraint is intra-point (True) or inter-point (False). Each callable\n",
    "                                      takes a tensor `X` of shape [q, d] (where `d` is the dimension of the input space\n",
    "                                      and `q` can be 1 or more representing different design points) and returns a scalar\n",
    "                                      that should be non-negative if the constraint is satisfied.\n",
    "    \"\"\"\n",
    "\n",
    "    def constraint(X):\n",
    "        \"\"\"\n",
    "        Evaluates all constraints for a batch of design points.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): A tensor of shape [q, d] (where `d` is the dimension of the input space and `q` can be 1 or more\n",
    "                              representing different design points).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape [q, m] (where `m` is the number of output dimensions) containing the evaluated\n",
    "                          constraints.\n",
    "        \"\"\"\n",
    "        # Compute posterior at X\n",
    "        X = X.unsqueeze(0)\n",
    "        posterior = model.posterior(X)\n",
    "        mean = posterior.mean\n",
    "        variance = posterior.variance\n",
    "        ucb = mean + beta * variance.sqrt()  # Compute the UCB\n",
    "\n",
    "        # Evaluate all constraints and return the difference from thresholds\n",
    "        return ucb - thresholds\n",
    "\n",
    "    # Create a list of constraints for each output dimension, all set as intra-point since they evaluate individually\n",
    "    constraints = [\n",
    "        (lambda X, i=i: constraint(X)[:, i], True) for i in range(thresholds.size(0))\n",
    "    ]\n",
    "\n",
    "    return constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample_on_n_sphere(N, R):\n",
    "    # Return a single sample of a vector of dimension N\n",
    "    # with a uniform distribution on the (N-1)-Sphere surface of radius R.\n",
    "    # RATIONALE: https://mathworld.wolfram.com/HyperspherePointPicking.html\n",
    "\n",
    "    # Generate a normally distributed point\n",
    "    X = torch.randn(N)\n",
    "\n",
    "    # Normalize this point to the surface of the sphere, then scale by radius R\n",
    "    return R * X / torch.norm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main BO Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel picking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import HV, violation\n",
    "from gauche.kernels.fingerprint_kernels.tanimoto_kernel import TanimotoKernel\n",
    "\n",
    "base = TanimotoKernel()\n",
    "covar_module = ScaleKernel(\n",
    "    base_kernel=base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "\n",
    "c = 0\n",
    "noise = 0.005\n",
    "print(\"0\" * 50)\n",
    "random_seeds = [\n",
    "    83810,\n",
    "    14592,\n",
    "    3278,\n",
    "    97196,\n",
    "    36048,\n",
    "    32098,\n",
    "    29256,\n",
    "    18289,\n",
    "    96530,\n",
    "    13434,\n",
    "    88696,\n",
    "    97080,\n",
    "    71482,\n",
    "    11395,\n",
    "    77397,\n",
    "    55302,\n",
    "    4165,\n",
    "    3905,\n",
    "    12280,\n",
    "    28657,\n",
    "    30495,\n",
    "    66237,\n",
    "    78907,\n",
    "    3478,\n",
    "    73563,\n",
    "    26062,\n",
    "    93850,\n",
    "    85181,\n",
    "    91924,\n",
    "    71426,\n",
    "    54987,\n",
    "    28893,\n",
    "    58878,\n",
    "    77236,\n",
    "    36463,\n",
    "    851,\n",
    "    99458,\n",
    "    20926,\n",
    "    91506,\n",
    "    55392,\n",
    "    44597,\n",
    "    36421,\n",
    "    20379,\n",
    "    28221,\n",
    "    44118,\n",
    "    13396,\n",
    "    12156,\n",
    "    49797,\n",
    "    12676,\n",
    "    47052,\n",
    "]\n",
    "declared = False\n",
    "for seed in random_seeds[:10]:\n",
    "    target = torch.load(\"target_ESOL.pt\")\n",
    "    domain = torch.load(\"domain_ESOL.pt\")\n",
    "    torch.manual_seed(seed)\n",
    "    ind = generate_initial_data(64)\n",
    "    train_X = domain[ind, :].to(torch.float64)\n",
    "    train_Y = target[ind, :].to(torch.float64)\n",
    "    train_Xr = train_X\n",
    "    train_Yr = train_Y\n",
    "    \"\"\"mask = [True]*target.shape[0]\n",
    "    mask[tuple(ind)] = False\n",
    "    target = target[mask,:]\n",
    "    domain = domain[mask,:]\"\"\"\n",
    "    thresholds = torch.tensor([2.5, 0.5, 55, -4], dtype=torch.float64)\n",
    "    Hpv = []\n",
    "    Hpvr = []\n",
    "    Acq = []\n",
    "    print(f\"round:0 {HV(Y = train_Y, ref = thresholds)}\")\n",
    "    NUM_ITER = 60\n",
    "    for batch in range(NUM_ITER):\n",
    "        model_list = []\n",
    "        m = 4\n",
    "        for i in range(m):\n",
    "            current_model = SingleTaskGP(\n",
    "                train_X=train_X,\n",
    "                train_Y=train_Y[:, i].unsqueeze(-1),\n",
    "                outcome_transform=Standardize(m=1),\n",
    "                train_Yvar=torch.zeros((train_X.shape[0], 1)) + noise**2,\n",
    "                covar_module=covar_module,\n",
    "            )\n",
    "            model_list.append(current_model)\n",
    "        model = ModelListGP(*model_list)\n",
    "        mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "        try:\n",
    "            fit_gpytorch_mll(mll)\n",
    "        except:\n",
    "            pass\n",
    "        # t1 = time.monotonic()\n",
    "        # sample theta from distribution\n",
    "        theta = get_random_sample_on_n_sphere(m, 1).abs()\n",
    "        beta = 0.05 * math.log(1 + batch)\n",
    "        beta_const = beta\n",
    "\n",
    "        # auxiliary acquisition\n",
    "        def const(X):\n",
    "            posterior = model.posterior(X)\n",
    "            mean = posterior.mean\n",
    "            variance = posterior.variance\n",
    "            ucb_const = mean + beta_const * variance.sqrt()\n",
    "            ucb_const = ucb_const\n",
    "            return ucb_const\n",
    "\n",
    "        vio = violation(const(domain), thresholds)\n",
    "        feasi_ind = vio == 0\n",
    "\n",
    "        # t2 = time.monotonic()\n",
    "        # create acquisition function\n",
    "        HVUCB = HyperVolumeScalarizedUCB(\n",
    "            model=model, beta=torch.tensor(beta), theta=theta, ref=thresholds\n",
    "        )\n",
    "        # optimize constraint function\n",
    "        candidate, acq_scalar = optimize_acqf_discrete(\n",
    "            acq_function=HVUCB, q=1, choices=domain[feasi_ind, :]\n",
    "        )\n",
    "        candidater = domain[random.sample(range(target.shape[0]), 1)[0], :].unsqueeze(0)\n",
    "        # t3 = time.monotonic()\n",
    "        # update data\n",
    "        train_X = torch.cat([train_X, candidate], dim=0)\n",
    "        train_Y = torch.cat(\n",
    "            [train_Y, target[test_f(candidate, domain), :].unsqueeze(0)], dim=0\n",
    "        )\n",
    "        train_Xr = torch.cat([train_Xr, candidater], dim=0)\n",
    "        train_Yr = torch.cat(\n",
    "            [train_Yr, target[test_f(candidater, domain), :].unsqueeze(0)], dim=0\n",
    "        )\n",
    "        hv = HV(Y=train_Y, ref=thresholds)\n",
    "        hvr = HV(Y=train_Yr, ref=thresholds)\n",
    "        print(f\"round: {batch+1}\", hv, test_f(candidate, domain), hvr)\n",
    "        Hpv.append(hv)\n",
    "        Hpvr.append(hvr)\n",
    "        Acq.append(acq_scalar)\n",
    "        ###prune candidate###\n",
    "        mask = torch.tensor([True] * target.shape[0])\n",
    "        mask[test_f(candidate, domain)] = False\n",
    "        domain = domain[mask, :]\n",
    "        target = target[mask, :]\n",
    "        ####################\n",
    "    if not declared:\n",
    "        c += 1\n",
    "        torch.save(torch.tensor(Hpv), f\"hv_ESOL_cmobo_0.05_{c}.pt\")\n",
    "        # torch.save(torch.tensor(Hpvr), f'hv_random_caco_0.1_{c}.pt')\n",
    "        torch.save(train_Y, f\"obj_ESOL_cmobo_0.05_{c}.pt\")\n",
    "        # torch.save(train_Yr, f'obj_caco_rand_0.5_{c}.pt')\n",
    "        print(\"o\", end=\"\")\n",
    "    else:\n",
    "        print(\"*\", end=\"\")\n",
    "    declared = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
